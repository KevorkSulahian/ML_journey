{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ccf4528",
   "metadata": {},
   "source": [
    "## Bedrcok Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca9c6498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of a 'hello world' program is to provide a simple introduction to the syntax and basic structure of a programming language.\n"
     ]
    }
   ],
   "source": [
    "import boto3, json\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "session = boto3.Session(profile_name=\"bedrock-dev\")\n",
    "client = session.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "model_id = \"amazon.nova-lite-v1:0\"\n",
    "user_message = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\"maxTokens\": 512, \"temperature\": 0.5, \"topP\": 0.9},\n",
    "    )\n",
    "\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a375bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provectus is located in Austin, TX, and has offices in the US and Ukraine.\n",
      "Sources: {'doc2', 'doc3', 'doc1'}\n"
     ]
    }
   ],
   "source": [
    "### For Chroma\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from openai import OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "CHAT_MODEL = \"gpt-4o-mini\"\n",
    "EMB_MODEL  = \"text-embedding-3-small\"\n",
    "\n",
    "CORPUS = [\n",
    "    (\"doc1\", \"\"\"Provectus is an AI/ML company focusing on GenAI, RAG, and agentic systems.\n",
    "They specialize in AWS Bedrock, OpenSearch, and reproducible ML pipelines.\"\"\"),\n",
    "    (\"doc2\", \"\"\"The ML Engineer role requires experience with LLMs in production, RAG, agentic systems,\n",
    "classification/regression tasks, and solid Python + Docker skills.\"\"\"),\n",
    "    (\"doc3\", \"\"\"Provectus is located in Austin, TX, and has offices in the US and Ukraine.\"\"\"),\n",
    "]\n",
    "\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts, metas = [], []\n",
    "for doc_id, text in CORPUS:\n",
    "    for ch in splitter.split_text(text):\n",
    "        texts.append(ch) \n",
    "        metas.append({\"source\": doc_id})\n",
    "\n",
    "\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vs = FAISS.from_texts(texts, emb, metadatas=metas)\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n",
    "res = qa.invoke({\"query\": \"Where is provectus located?\"})\n",
    "print(res[\"result\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ead9d",
   "metadata": {},
   "source": [
    "LangChain RAg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13ae087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda:0\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'SA, Canada, and Mozambique', 'sources': ['Provectus is an AI and ML consulting company focused on building cutting-edge solutions.\\n    They sp...'], 'confidence': 'high'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, model_name=\"gpt-3.5-turbo\", embedding_model=\"all-MiniLM-L6-v2\"):\n",
    "        self.llm = ChatOpenAI(model=model_name, temperature=0)\n",
    "        self.embedding = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "        self.vectorstore = None\n",
    "        self.retriever = None\n",
    "        self.qa_chain = None\n",
    "\n",
    "    def load_and_chunk_text(self, text: str, chunk_size=500, chunk_overlap=50):\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        return splitter.split_text(text)\n",
    "\n",
    "    def build_index(self, texts: List[str]):\n",
    "        self.vectorstore = FAISS.from_texts(texts, self.embedding)\n",
    "        self.retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    def setup_chain(self):\n",
    "        template = \"\"\"Answer the question based only on the following context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        If you don't know, say \"I don't know\".\n",
    "        Answer:\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        self.qa_chain = (\n",
    "            {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def query(self, question: str) -> Dict:\n",
    "        try:\n",
    "            if not self.qa_chain:\n",
    "                raise ValueError(\"Chain not initialized. Call setup_chain() first.\")\n",
    "            answer = self.qa_chain.invoke(question)\n",
    "            # Get sources (optional)\n",
    "            docs = self.retriever.invoke(question)\n",
    "            sources = [doc.page_content[:100] + \"...\" for doc in docs]\n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"sources\": sources,\n",
    "                \"confidence\": \"high\" if len(answer) > 10 else \"low\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Query failed: {e}\")\n",
    "            return {\"answer\": \"Error occurred\", \"sources\": [], \"confidence\": \"none\"}\n",
    "\n",
    "# === USAGE ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample text\n",
    "    sample_text = \"\"\"\n",
    "    Provectus is an AI and ML consulting company focused on building cutting-edge solutions.\n",
    "    They specialize in GenAI, RAG, agentic systems, and cloud-based ML pipelines.\n",
    "    Offices are located in SA, Canada, and Mozambique.\n",
    "    \"\"\"\n",
    "\n",
    "    rag = RAGSystem()\n",
    "    chunks = rag.load_and_chunk_text(sample_text)\n",
    "    rag.build_index(chunks)\n",
    "    rag.setup_chain()\n",
    "\n",
    "    result = rag.query(\"Where is Provectus located?\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228fbcae",
   "metadata": {},
   "source": [
    "Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c547eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `count_letters` with `{'text': 'strawberry'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m{'s': 1, 't': 1, 'r': 3, 'a': 1, 'w': 1, 'b': 1, 'e': 1, 'y': 1}\u001b[0m\u001b[32;1m\u001b[1;3mThere are 3 'r's in the word \"strawberry\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'output': 'There are 3 \\'r\\'s in the word \"strawberry\".', 'steps': 0, 'status': 'success'}\n"
     ]
    }
   ],
   "source": [
    "# agent_template.py\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import Dict\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Evaluates a mathematical expression. Use for +, -, *, /, **.\"\"\"\n",
    "    try:\n",
    "        # Safe eval for demo (in prod, use ast.literal_eval or numexpr)\n",
    "        result = eval(expression)\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "@tool\n",
    "def count_letters(text: str) -> dict:\n",
    "    \"\"\"Count letters from text and return a dictionary {letter: count}.\"\"\"\n",
    "    counts = {}\n",
    "    for char in text:\n",
    "        if char.isalpha():  # Only count letters\n",
    "            char = char.lower()\n",
    "            counts[char] = counts.get(char, 0) + 1\n",
    "    return counts\n",
    "\n",
    "tools = [calculator, count_letters]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant with tools.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "def run_agent(query: str) -> Dict:\n",
    "    try:\n",
    "        response = agent_executor.invoke({\"input\": query})\n",
    "        return {\n",
    "            \"output\": response[\"output\"],\n",
    "            \"steps\": len(response.get(\"intermediate_steps\", [])),\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"output\": f\"Agent failed: {e}\", \"steps\": 0, \"status\": \"error\"}\n",
    "\n",
    "# === USAGE ===\n",
    "if __name__ == \"__main__\":\n",
    "    result = run_agent(\"how many r's are in strawberry?\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d37d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kevor\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'SentimentResponse' has no attribute 'model_json_schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     41\u001b[0m     classifier \u001b[38;5;241m=\u001b[39m Classifier()\n\u001b[1;32m---> 42\u001b[0m     result \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mclassify_sentiment(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love this product! It changed my life.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(result, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[1;32mIn[10], line 30\u001b[0m, in \u001b[0;36mClassifier.classify_sentiment\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassify_sentiment\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[0;32m     23\u001b[0m     prompt_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124m    Classify sentiment of this text. Be conservative with confidence.\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124m    Text: \u001b[39m\u001b[38;5;132;01m{text}\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{format_instructions}\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     28\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m prompt_template\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     29\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m---> 30\u001b[0m         format_instructions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mget_format_instructions()\n\u001b[0;32m     31\u001b[0m     )\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39minvoke(prompt)\n",
      "File \u001b[1;32mc:\\Users\\kevor\\anaconda3\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:90\u001b[0m, in \u001b[0;36mPydanticOutputParser.get_format_instructions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the format instructions for the JSON output.\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m    The format instructions for the JSON output.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Copy schema to avoid altering original Pydantic schema.\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object\u001b[38;5;241m.\u001b[39mmodel_json_schema()\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Remove extraneous fields.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m reduced_schema \u001b[38;5;241m=\u001b[39m schema\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'SentimentResponse' has no attribute 'model_json_schema'"
     ]
    }
   ],
   "source": [
    "# llm_wrapper_template.py\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from typing import Dict, Optional\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class LLMWrapper:\n",
    "    def __init__(self, model=\"gpt-3.5-turbo\", max_retries=3):\n",
    "        self.llm = ChatOpenAI(model=model, temperature=0.7)\n",
    "        self.max_retries = max_retries\n",
    "        self.metrics = []\n",
    "\n",
    "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))\n",
    "    def _safe_invoke(self, prompt: str):\n",
    "        return self.llm.invoke(prompt)\n",
    "\n",
    "    def generate(self, prompt: str, task_id: str = \"default\") -> Dict:\n",
    "        start = time.time()\n",
    "        try:\n",
    "            response = self._safe_invoke(prompt)\n",
    "            latency = time.time() - start\n",
    "            tokens_in = len(prompt.split())  # rough approx\n",
    "            cost = tokens_in * 0.000001  # placeholder\n",
    "\n",
    "            result = {\n",
    "                \"output\": response.content,\n",
    "                \"latency\": round(latency, 2),\n",
    "                \"tokens_in\": tokens_in,\n",
    "                \"cost\": round(cost, 6),\n",
    "                \"status\": \"success\",\n",
    "                \"task_id\": task_id\n",
    "            }\n",
    "            self.metrics.append(result)\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            latency = time.time() - start\n",
    "            result = {\n",
    "                \"output\": \"\",\n",
    "                \"latency\": round(latency, 2),\n",
    "                \"tokens_in\": len(prompt.split()),\n",
    "                \"cost\": 0,\n",
    "                \"status\": f\"error: {str(e)}\",\n",
    "                \"task_id\": task_id\n",
    "            }\n",
    "            self.metrics.append(result)\n",
    "            logging.error(f\"LLM call failed: {e}\")\n",
    "            return result\n",
    "\n",
    "    def get_metrics(self):\n",
    "        return self.metrics\n",
    "\n",
    "# === USAGE ===\n",
    "if __name__ == \"__main__\":\n",
    "    wrapper = LLMWrapper()\n",
    "    result = wrapper.generate(\"Explain RAG in one sentence.\", \"task_1\")\n",
    "    print(result)\n",
    "    print(\"Metrics:\", wrapper.get_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168bec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bedrock_template.py\n",
    "import json\n",
    "import boto3\n",
    "import logging\n",
    "from typing import Dict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class BedrockClient:\n",
    "    def __init__(self, region=\"us-east-1\", model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\"):\n",
    "        self.client = boto3.client(\"bedrock-runtime\", region_name=region)\n",
    "        self.model_id = model_id\n",
    "\n",
    "    def invoke(self, prompt: str, max_tokens=500, temperature=0.7) -> Dict:\n",
    "        try:\n",
    "            # Claude 3 format\n",
    "            body = json.dumps({\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "                    }\n",
    "                ]\n",
    "            })\n",
    "\n",
    "            response = self.client.invoke_model(\n",
    "                modelId=self.model_id,\n",
    "                body=body\n",
    "            )\n",
    "\n",
    "            response_body = json.loads(response[\"body\"].read())\n",
    "            output = response_body[\"content\"][0][\"text\"]\n",
    "\n",
    "            return {\n",
    "                \"output\": output,\n",
    "                \"status\": \"success\",\n",
    "                \"model\": self.model_id\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Bedrock error: {e}\")\n",
    "            return {\"output\": \"\", \"status\": f\"error: {str(e)}\", \"model\": self.model_id}\n",
    "\n",
    "# === USAGE ===\n",
    "if __name__ == \"__main__\":\n",
    "    # ⚠️ Ensure AWS credentials are set in ~/.aws/credentials or env vars\n",
    "    bedrock = BedrockClient()\n",
    "    result = bedrock.invoke(\"Explain what RAG is in one sentence.\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526f9714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_template.py\n",
    "import re\n",
    "from typing import Dict\n",
    "\n",
    "class LLMEvaluator:\n",
    "    @staticmethod\n",
    "    def keyword_match(output: str, expected_keywords: list) -> Dict:\n",
    "        found = [kw for kw in expected_keywords if kw.lower() in output.lower()]\n",
    "        score = len(found) / len(expected_keywords) if expected_keywords else 0\n",
    "        return {\"metric\": \"keyword_recall\", \"score\": score, \"found\": found}\n",
    "\n",
    "    @staticmethod\n",
    "    def contains_forbidden(output: str, forbidden: list) -> bool:\n",
    "        return any(f.lower() in output.lower() for f in forbidden)\n",
    "\n",
    "    @staticmethod\n",
    "    def answer_length(output: str) -> Dict:\n",
    "        word_count = len(output.split())\n",
    "        return {\n",
    "            \"metric\": \"length\",\n",
    "            \"word_count\": word_count,\n",
    "            \"category\": \"short\" if word_count < 10 else \"medium\" if word_count < 50 else \"long\"\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def run_all(output: str, expected_keywords=[], forbidden=[]) -> Dict:\n",
    "        return {\n",
    "            \"keyword_eval\": LLMEvaluator.keyword_match(output, expected_keywords),\n",
    "            \"forbidden_found\": LLMEvaluator.contains_forbidden(output, forbidden),\n",
    "            \"length_eval\": LLMEvaluator.answer_length(output)\n",
    "        }\n",
    "\n",
    "# === USAGE ===\n",
    "if __name__ == \"__main__\":\n",
    "    output = \"RAG stands for Retrieval-Augmented Generation. It combines retrieval and generation.\"\n",
    "    eval_result = LLMEvaluator.run_all(\n",
    "        output,\n",
    "        expected_keywords=[\"retrieval\", \"generation\", \"RAG\"],\n",
    "        forbidden=[\"I don't know\", \"sorry\"]\n",
    "    )\n",
    "    print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_template.py\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "\n",
    "class ExperimentTracker:\n",
    "    def __init__(self, experiment_name: str):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.runs = []\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def log_run(self, params: Dict, result: Dict, notes: str = \"\"):\n",
    "        run = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"params\": params,\n",
    "            \"result\": result,\n",
    "            \"notes\": notes,\n",
    "            \"duration_sec\": time.time() - self.start_time\n",
    "        }\n",
    "        self.runs.append(run)\n",
    "        print(f\"[Experiment] Logged run with params: {params}\")\n",
    "\n",
    "    def save_to_file(self, filename: str = None):\n",
    "        if not filename:\n",
    "            filename = f\"{self.experiment_name}_{int(time.time())}.json\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.runs, f, indent=2)\n",
    "        print(f\"Saved {len(self.runs)} runs to {filename}\")\n",
    "\n",
    "    def get_summary(self):\n",
    "        successes = sum(1 for r in self.runs if r[\"result\"].get(\"status\") == \"success\")\n",
    "        total = len(self.runs)\n",
    "        avg_latency = sum(r[\"result\"].get(\"latency\", 0) for r in self.runs) / total if total > 0 else 0\n",
    "        return {\n",
    "            \"total_runs\": total,\n",
    "            \"success_rate\": successes / total if total > 0 else 0,\n",
    "            \"avg_latency\": round(avg_latency, 2)\n",
    "        }\n",
    "\n",
    "# === USAGE ===\n",
    "if __name__ == \"__main__\":\n",
    "    tracker = ExperimentTracker(\"rag_comparison_v1\")\n",
    "\n",
    "    # Simulate 2 runs\n",
    "    tracker.log_run(\n",
    "        params={\"model\": \"gpt-3.5\", \"chunk_size\": 500},\n",
    "        result={\"answer\": \"Yerevan\", \"latency\": 2.1, \"status\": \"success\"},\n",
    "        notes=\"Baseline RAG\"\n",
    "    )\n",
    "\n",
    "    tracker.log_run(\n",
    "        params={\"model\": \"claude-sonnet\", \"chunk_size\": 300},\n",
    "        result={\"answer\": \"Belgrade\", \"latency\": 3.5, \"status\": \"success\"},\n",
    "        notes=\"Smaller chunks\"\n",
    "    )\n",
    "\n",
    "    print(tracker.get_summary())\n",
    "    tracker.save_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59cfac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse, glob\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d79527",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "    Provectus is an AI and ML consulting company focused on building cutting-edge solutions.\n",
    "    They specialize in GenAI, RAG, agentic systems, and cloud-based ML pipelines.\n",
    "    Offices are located in SA, Canada, and Mozambique.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bac032",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636af6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chroma-rules",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
