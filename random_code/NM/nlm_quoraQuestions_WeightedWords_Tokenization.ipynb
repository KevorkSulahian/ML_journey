{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aa9763f1",
      "metadata": {
        "id": "aa9763f1"
      },
      "source": [
        "To do: Add stemming, remove common words, confusion matrix, xgboost, remove extra symbols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af55508f",
      "metadata": {
        "id": "af55508f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import re\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aaa4512",
      "metadata": {
        "id": "6aaa4512"
      },
      "source": [
        "Import Training Data and Filter Empty/Invalid Entries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MDclnFLCGkxD",
      "metadata": {
        "id": "MDclnFLCGkxD"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dfc45f6",
      "metadata": {
        "id": "2dfc45f6"
      },
      "outputs": [],
      "source": [
        "N = 5000\n",
        "DATA_PATH = \"https://raw.githubusercontent.com/KevorkSulahian/ML_journey/main/random_code/NM/train.csv.zip\"\n",
        "\n",
        "train = pd.read_csv(DATA_PATH, compression=\"zip\").iloc[:N]\n",
        "trainFiltered = train[\n",
        "    train['question1'].apply(lambda x: isinstance(x, str) and x.strip() != '') &\n",
        "    train['question2'].apply(lambda x: isinstance(x, str) and x.strip() != '')\n",
        "].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "714b61a1",
      "metadata": {
        "id": "714b61a1"
      },
      "source": [
        "Import WordNet Synonyms and Stemming Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f29155ef",
      "metadata": {
        "id": "f29155ef",
        "outputId": "473eab4a-5420-494c-8e07-8ae19410f524"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('stopwords')\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad31b404",
      "metadata": {
        "id": "ad31b404"
      },
      "source": [
        "Create DataFrame of Synsets (canonical synonyms to replace words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2b40105",
      "metadata": {
        "id": "d2b40105"
      },
      "outputs": [],
      "source": [
        "def makeSynonymDf(synsets):\n",
        "    word_list = []\n",
        "    wordSynonyms = []\n",
        "\n",
        "    for synset in synsets:\n",
        "        word = synset.name().split('.')[0].replace('_', ' ')\n",
        "        synonyms = [lemma.replace('_', ' ') for lemma in synset.lemma_names()]\n",
        "\n",
        "        word_list.append(word)\n",
        "        wordSynonyms.append(synonyms)\n",
        "\n",
        "    wordDf = pd.DataFrame({'word': word_list, 'synonyms': wordSynonyms})\n",
        "    return wordDf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f7c6dca",
      "metadata": {
        "id": "4f7c6dca"
      },
      "outputs": [],
      "source": [
        "def build_synonym_map(df):\n",
        "    synonym_map = {}\n",
        "    for _, row in df.iterrows():\n",
        "        syn_list = row['synonyms']\n",
        "        if isinstance(syn_list, list) and syn_list:\n",
        "            canonical = syn_list[0].lower()  # first word is the canonical one\n",
        "            for word in syn_list:\n",
        "                synonym_map[word.lower()] = canonical\n",
        "    return synonym_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2f3c18c",
      "metadata": {
        "id": "e2f3c18c"
      },
      "outputs": [],
      "source": [
        "'''nounSynsets = list(wn.all_synsets('n'))\n",
        "nounDf = makeSynonymDf(nounSynsets)\n",
        "\n",
        "verbSynsets = list(wn.all_synsets('v'))\n",
        "verbDf = makeSynonymDf(verbSynsets)\n",
        "\n",
        "adjSynsets = list(wn.all_synsets('a'))\n",
        "adjDf = makeSynonymDf(adjSynsets)'''\n",
        "\n",
        "allSynsets = list(wn.all_synsets())\n",
        "wordDf = makeSynonymDf(allSynsets)\n",
        "\n",
        "english_stopwords = set(stopwords.words(\"english\"))\n",
        "query_words = {'who', 'what', 'where', 'when', 'why', 'how', 'which', 'will', 'whose', 'can'}\n",
        "negation_words = {\"no\", \"not\", \"never\", \"none\", \"neither\", \"nor\", \"nothing\", \"cannot\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84e9c00b",
      "metadata": {
        "id": "84e9c00b"
      },
      "outputs": [],
      "source": [
        "synonym_map = build_synonym_map(wordDf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fcedcb1",
      "metadata": {
        "id": "3fcedcb1"
      },
      "source": [
        "Process String and Tokenize Words with Stemming+Canonical Synonym Replacement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b56df21b",
      "metadata": {
        "id": "b56df21b"
      },
      "outputs": [],
      "source": [
        "def normalize_and_tokenize_with_stemming(question, synonym_map):\n",
        "    question = question.lower()\n",
        "    question = re.sub(r\"n't\", \" not\", question)\n",
        "\n",
        "    # Remove all non-alphanumeric characters (symbols like #, @, $, etc.)\n",
        "    question = re.sub(r'[^a-z0-9\\s]', '', question)\n",
        "    tokens = re.findall(r'\\b\\w+\\b', question)\n",
        "\n",
        "    # Remove stopwords\n",
        "    filtered_tokens = {\n",
        "        w for w in tokens if w not in english_stopwords or w in negation_words or w in query_words\n",
        "    }\n",
        "\n",
        "    stemmed = [\n",
        "        w if w in negation_words or w in query_words else stemmer.stem(w)\n",
        "        for w in filtered_tokens\n",
        "    ]\n",
        "\n",
        "    # Replace with canonical synonyms only if not in negation/query words\n",
        "    norm_tokens = [\n",
        "        w if w in negation_words or w in query_words else synonym_map.get(w, w)\n",
        "        for w in stemmed\n",
        "        ]\n",
        "\n",
        "    tags = pos_tag(filtered_tokens)\n",
        "    nouns = {word for word, tag in tags if tag.startswith('NN')}\n",
        "    verbs = {word for word, tag in tags if tag.startswith('VB')}\n",
        "    adjectives = {word for word, tag in tags if tag.startswith('JJ')}\n",
        "    stemmedNouns = [stemmer.stem(w)for w in nouns]\n",
        "    stemmedVerbs = [stemmer.stem(w)for w in verbs]\n",
        "    stemmedAdjectives = [stemmer.stem(w)for w in adjectives]\n",
        "\n",
        "    return set(norm_tokens), set(stemmedNouns), set(stemmedVerbs), set(stemmedAdjectives)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31420b25",
      "metadata": {},
      "outputs": [],
      "source": [
        "trainFiltered.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21a94b1d",
      "metadata": {
        "id": "21a94b1d",
        "outputId": "9762b213-7409-47b0-8395-61ceeeadec16"
      },
      "outputs": [],
      "source": [
        "trainNormalized = trainFiltered.copy()\n",
        "import nltk\n",
        "nltk.data.path.append('C:\\\\Users\\\\kevor\\\\nltk_data')  # Adjust the path to where NLTK data is stored\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger', force=True)\n",
        "trainNormalized[['tokens1', 'nouns1', 'verbs1', 'adjectives1']] = trainNormalized['question1'].apply(\n",
        "    lambda question: pd.Series(normalize_and_tokenize_with_stemming(question, synonym_map))\n",
        ")\n",
        "\n",
        "trainNormalized[['tokens2', 'nouns2', 'verbs2', 'adjectives2']] = trainNormalized['question2'].apply(\n",
        "    lambda question: pd.Series(normalize_and_tokenize_with_stemming(question, synonym_map))\n",
        ")\n",
        "\n",
        "trainNormalized = trainNormalized[\n",
        "    trainNormalized['tokens1'].apply(lambda x: isinstance(x, set) and len(x) > 0) &\n",
        "    trainNormalized['tokens2'].apply(lambda x: isinstance(x, set) and len(x) > 0)\n",
        "].reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1212faff",
      "metadata": {},
      "outputs": [],
      "source": [
        "trainNormalized.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0472e33",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, gc, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse import vstack, hstack\n",
        "import torch\n",
        "from sentence_transformers import SparseEncoder, SentenceTransformer, CrossEncoder\n",
        "from sklearn.decomposition import TruncatedSVD, PCA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.metrics import edit_distance\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37b1cda7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, gc, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse import vstack, hstack\n",
        "import torch\n",
        "\n",
        "from sentence_transformers import SparseEncoder, SentenceTransformer, CrossEncoder\n",
        "from sklearn.decomposition import TruncatedSVD, PCA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.metrics import edit_distance\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def run_duplicate_pipeline(\n",
        "    q1_list: list,\n",
        "    q2_list: list,\n",
        "    y: np.ndarray,\n",
        "    topk: int = 100,\n",
        "    n_svd: int = 1500,\n",
        "    n_pca: int = 350,\n",
        "    max_features: int = 50000,\n",
        "    random_state: int = 42,\n",
        "    device: str = None,\n",
        "    output_dir: str = \"results\"\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Runs the duplicate-question pipeline with an 80/20 train/test split,\n",
        "    saves all diagnostic plots to `output_dir`, and returns metrics + models.\n",
        "\n",
        "    Args:\n",
        "        q1_list, q2_list: full lists of question texts.\n",
        "        y: full label array.\n",
        "        topk: keep top-k non-zero entries per SPLADE batch.\n",
        "        n_svd: components for TruncatedSVD.\n",
        "        n_pca: components for PCA.\n",
        "        max_features: for TF-IDF.\n",
        "        random_state: seed.\n",
        "        device: 'cuda' or 'cpu'. Auto-detect if None.\n",
        "        output_dir: directory to save all plots.\n",
        "    Returns:\n",
        "        dict containing:\n",
        "          - stack_model\n",
        "          - auc_xe_train, auc_stack_train\n",
        "          - auc_xe_test, auc_stack_test\n",
        "          - X_train, X_test\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # 0. Train/test split (80/20)\n",
        "    q1_train, q1_test, q2_train, q2_test, y_train, y_test = train_test_split(\n",
        "        q1_list, q2_list, y, test_size=0.2, \n",
        "        stratify=y, random_state=random_state\n",
        "    )\n",
        "    N = len(q1_train)\n",
        "\n",
        "    # 1. Device\n",
        "    if device is None:\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # 2. Instantiate models\n",
        "    sparse_model = SparseEncoder(\"naver/splade-cocondenser-ensembledistil\",\n",
        "                                 device=device, trust_remote_code=True)\n",
        "    dense_model  = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
        "    xe_model     = CrossEncoder(\"cross-encoder/quora-distilroberta-base\",\n",
        "                                device=device, num_labels=1)\n",
        "    tfidf        = TfidfVectorizer(max_features=max_features, ngram_range=(1,2))\n",
        "\n",
        "    # 3. Helper: sparse CSR\n",
        "    def get_sparse_csr(sentences, model):\n",
        "        chunks = []\n",
        "        for i in tqdm(range(0, len(sentences), 128), desc=\"Sparse encode\"):\n",
        "            batch = sentences[i:i+128]\n",
        "            emb = model.encode(batch, batch_size=128, convert_to_numpy=False)\n",
        "            coo = emb.coalesce()\n",
        "            rows, cols = coo.indices()[0].cpu().numpy(), coo.indices()[1].cpu().numpy()\n",
        "            vals = coo.values().cpu().numpy()\n",
        "            if topk:\n",
        "                idx = np.argsort(vals)[-topk:]\n",
        "                rows, cols, vals = rows[idx], cols[idx], vals[idx]\n",
        "            chunks.append(sp.coo_matrix((vals,(rows,cols)), shape=coo.shape).tocsr())\n",
        "            del emb, coo, rows, cols, vals\n",
        "            torch.cuda.empty_cache()\n",
        "        return vstack(chunks)\n",
        "\n",
        "    ### — TRAIN FEATURE EXTRACTION — ###\n",
        "    flat_train = [q for pair in zip(q1_train, q2_train) for q in pair]\n",
        "\n",
        "    # Sparse + SVD\n",
        "    sparse_full = get_sparse_csr(flat_train, sparse_model)\n",
        "    s1, s2 = sparse_full[:N], sparse_full[N:]\n",
        "    svd = TruncatedSVD(n_components=n_svd, random_state=random_state)\n",
        "    X_sp_train = svd.fit_transform(hstack([abs(s1-s2), s1.multiply(s2)]))\n",
        "\n",
        "    # Dense + PCA\n",
        "    emb_dense = dense_model.encode(flat_train,\n",
        "                                   batch_size=64,\n",
        "                                   convert_to_tensor=True,\n",
        "                                   device=device)\n",
        "    d1, d2 = emb_dense[:N].cpu().numpy(), emb_dense[N:].cpu().numpy()\n",
        "    pca = PCA(n_components=n_pca, random_state=random_state)\n",
        "    X_dn_train = pca.fit_transform(np.hstack([np.abs(d1-d2), d1*d2]))\n",
        "    cosims_train = np.diag(cosine_similarity(d1, d2))\n",
        "\n",
        "    # TF-IDF + lexical\n",
        "    tfidf.fit(flat_train)\n",
        "    v1_t, v2_t = tfidf.transform(q1_train), tfidf.transform(q2_train)\n",
        "    tfidf_sim_train = cosine_similarity(v1_t, v2_t).diagonal()\n",
        "    jacc_train, lev_train = [], []\n",
        "    for a,b in zip(q1_train, q2_train):\n",
        "        t1, t2 = set(word_tokenize(a.lower())), set(word_tokenize(b.lower()))\n",
        "        jacc_train.append(len(t1&t2)/(len(t1|t2)+1e-8))\n",
        "        lev_train.append(1 - edit_distance(a,b)/max(len(a),len(b),1))\n",
        "    jacc_train, lev_train = np.array(jacc_train), np.array(lev_train)\n",
        "\n",
        "    # Cross-Encoder\n",
        "    scores_train = xe_model.predict(list(zip(q1_train, q2_train)), batch_size=32)\n",
        "    xe_feat_train = (scores_train - scores_train.min())/(scores_train.max()-scores_train.min())\n",
        "    auc_xe_train = roc_auc_score(y_train, xe_feat_train)\n",
        "\n",
        "    # Save TRAIN diagnostics\n",
        "    def save_fig(fn):\n",
        "        fig = plt.gcf()\n",
        "        fig.savefig(os.path.join(output_dir, fn), bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "\n",
        "    # Class balance\n",
        "    pd.Series(y_train).value_counts().plot.bar(title=\"Train Duplicate vs Non-dup\")\n",
        "    save_fig(\"train_class_balance.png\")\n",
        "\n",
        "    # Length hists\n",
        "    for name, lst in zip([\"q1\",\"q2\"], [q1_train, q2_train]):\n",
        "        plt.hist([len(q) for q in lst], bins=30)\n",
        "        plt.title(f\"Train {name} lengths\")\n",
        "        save_fig(f\"train_len_{name}.png\")\n",
        "\n",
        "    # SPLADE sparsity\n",
        "    density = sparse_full.nnz/(sparse_full.shape[0]*sparse_full.shape[1])\n",
        "    # (we'll save the numeric density to a text file)\n",
        "    with open(os.path.join(output_dir, \"train_splade_density.txt\"), \"w\") as f:\n",
        "        f.write(f\"{density:.6f}\")\n",
        "\n",
        "    rows, cols = sparse_full.nonzero()\n",
        "    sel = np.random.rand(len(rows))<0.01\n",
        "    plt.scatter(cols[sel], rows[sel], s=0.5, alpha=0.1)\n",
        "    plt.title(\"Train SPLADE 1% sparsity\")\n",
        "    save_fig(\"train_splade_pattern.png\")\n",
        "    plt.hist(cols, bins=1000)\n",
        "    plt.title(\"Train Active Vocab IDs\")\n",
        "    save_fig(\"train_splade_vocab_hist.png\")\n",
        "    plt.plot(np.cumsum(svd.explained_variance_ratio_))\n",
        "    plt.title(\"Train Sparse SVD var\")\n",
        "    save_fig(\"train_svd_variance.png\")\n",
        "\n",
        "    # SBERT & PCA\n",
        "    plt.hist(cosims_train, bins=30)\n",
        "    plt.title(\"Train SBERT cosine sim\")\n",
        "    save_fig(\"train_cosine_sim.png\")\n",
        "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "    plt.title(\"Train PCA var\")\n",
        "    save_fig(\"train_pca_variance.png\")\n",
        "\n",
        "    # Lexical\n",
        "    for arr,name in [(tfidf_sim_train,\"tfidf\"), (jacc_train,\"jaccard\"), (lev_train,\"levenshtein\")]:\n",
        "        plt.hist(arr, bins=30)\n",
        "        plt.title(f\"Train {name} sim\")\n",
        "        save_fig(f\"train_{name}_hist.png\")\n",
        "\n",
        "    # Cross-Encoder\n",
        "    plt.hist(scores_train, bins=30)\n",
        "    plt.title(\"Train XE raw scores\")\n",
        "    save_fig(\"train_xe_raw.png\")\n",
        "    neg, pos = xe_feat_train[y_train==0], xe_feat_train[y_train==1]\n",
        "    plt.hist([neg,pos], bins=30, stacked=True)\n",
        "    plt.title(\"Train XE by label\")\n",
        "    save_fig(\"train_xe_by_label.png\")\n",
        "\n",
        "    # Assemble features\n",
        "    X_train = np.hstack([\n",
        "        X_sp_train,\n",
        "        X_dn_train,\n",
        "        tfidf_sim_train.reshape(-1,1),\n",
        "        jacc_train.reshape(-1,1),\n",
        "        lev_train.reshape(-1,1),\n",
        "        xe_feat_train.reshape(-1,1),\n",
        "    ])\n",
        "\n",
        "    # 4. Modeling (same as before)\n",
        "    CV = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
        "    param_dist = {\n",
        "        'n_estimators': [400,500], 'max_depth': [4,6,8],\n",
        "        'learning_rate': [0.01,0.05,0.1], 'subsample': [0.6,0.8,1.0],\n",
        "        'colsample_bytree': [0.6,0.8,1.0],\n",
        "        'scale_pos_weight': [1, sum(y_train==1)/sum(y_train==0)]\n",
        "    }\n",
        "    xgb_base = XGBClassifier(\n",
        "        tree_method = 'gpu_hist' if device=='cuda' else 'hist',\n",
        "        device      = device,\n",
        "        eval_metric = 'auc',\n",
        "        random_state= random_state\n",
        "    )\n",
        "    rs = RandomizedSearchCV(\n",
        "        xgb_base, param_dist, n_iter=10,\n",
        "        scoring='roc_auc', cv=CV, n_jobs=1, random_state=random_state\n",
        "    )\n",
        "    rs.fit(X_train, y_train)\n",
        "    best = rs.best_params_\n",
        "\n",
        "    estimators = [\n",
        "        ('lr', LogisticRegression(max_iter=1000, random_state=random_state)),\n",
        "        ('xgb', XGBClassifier(\n",
        "            **best,\n",
        "            tree_method = 'gpu_hist' if device=='cuda' else 'hist',\n",
        "            device      = device,\n",
        "            eval_metric = 'auc',\n",
        "            random_state= random_state\n",
        "        ))\n",
        "    ]\n",
        "    stack = StackingClassifier(\n",
        "        estimators       = estimators,\n",
        "        final_estimator  = XGBClassifier(\n",
        "            n_estimators = 100,\n",
        "            tree_method  = 'gpu_hist' if device=='cuda' else 'hist',\n",
        "            device       = device,\n",
        "            eval_metric  = 'auc',\n",
        "            random_state = random_state\n",
        "        ),\n",
        "        cv               = CV,\n",
        "        n_jobs           = -1\n",
        "    )\n",
        "    stack.fit(X_train, y_train)\n",
        "    probs_train = stack.predict_proba(X_train)[:,1]\n",
        "    auc_stack_train = roc_auc_score(y_train, probs_train)\n",
        "\n",
        "    # 5. TEST FEATURE EXTRACTION & EVAL\n",
        "    flat_test = [q for pair in zip(q1_test, q2_test) for q in pair]\n",
        "    sparse_full_t = get_sparse_csr(flat_test, sparse_model)\n",
        "    s1_t, s2_t = sparse_full_t[:len(q1_test)], sparse_full_t[len(q1_test):]\n",
        "    X_sp_test = svd.transform(hstack([abs(s1_t-s2_t), s1_t.multiply(s2_t)]))\n",
        "\n",
        "    emb_t = dense_model.encode(flat_test,\n",
        "                               batch_size=64,\n",
        "                               convert_to_tensor=True,\n",
        "                               device=device)\n",
        "    d1t, d2t = emb_t[:len(q1_test)].cpu().numpy(), emb_t[len(q1_test):].cpu().numpy()\n",
        "    X_dn_test = pca.transform(np.hstack([np.abs(d1t-d2t), d1t*d2t]))\n",
        "    cosims_test = np.diag(cosine_similarity(d1t, d2t))\n",
        "\n",
        "    v1_tt, v2_tt = tfidf.transform(q1_test), tfidf.transform(q2_test)\n",
        "    tfidf_sim_test = cosine_similarity(v1_tt, v2_tt).diagonal()\n",
        "    jacc_test, lev_test = [], []\n",
        "    for a,b in zip(q1_test, q2_test):\n",
        "        t1, t2 = set(word_tokenize(a.lower())), set(word_tokenize(b.lower()))\n",
        "        jacc_test.append(len(t1&t2)/(len(t1|t2)+1e-8))\n",
        "        lev_test.append(1 - edit_distance(a,b)/max(len(a),len(b),1))\n",
        "    jacc_test, lev_test = np.array(jacc_test), np.array(lev_test)\n",
        "\n",
        "    scores_test = xe_model.predict(list(zip(q1_test, q2_test)), batch_size=32)\n",
        "    xe_feat_test = (scores_test - scores_test.min())/(scores_test.max()-scores_test.min())\n",
        "    auc_xe_test = roc_auc_score(y_test, xe_feat_test)\n",
        "\n",
        "    X_test = np.hstack([\n",
        "        X_sp_test,\n",
        "        X_dn_test,\n",
        "        tfidf_sim_test.reshape(-1,1),\n",
        "        jacc_test.reshape(-1,1),\n",
        "        lev_test.reshape(-1,1),\n",
        "        xe_feat_test.reshape(-1,1),\n",
        "    ])\n",
        "    probs_test = stack.predict_proba(X_test)[:,1]\n",
        "    auc_stack_test = roc_auc_score(y_test, probs_test)\n",
        "\n",
        "    # Save TEST diagnostics\n",
        "    pd.Series(y_test).value_counts().plot.bar(title=\"Test Duplicate vs Non-dup\")\n",
        "    save_fig(\"test_class_balance.png\")\n",
        "\n",
        "    for name, lst in zip([\"q1\",\"q2\"], [q1_test, q2_test]):\n",
        "        plt.hist([len(q) for q in lst], bins=30)\n",
        "        plt.title(f\"Test {name} lengths\")\n",
        "        save_fig(f\"test_len_{name}.png\")\n",
        "\n",
        "    with open(os.path.join(output_dir, \"test_splade_density.txt\"), \"w\") as f:\n",
        "        f.write(f\"{sparse_full_t.nnz/(sparse_full_t.shape[0]*sparse_full_t.shape[1]):.6f}\")\n",
        "\n",
        "    # sparsity pattern\n",
        "    r_t, c_t = sparse_full_t.nonzero()\n",
        "    sel_t = np.random.rand(len(r_t))<0.01\n",
        "    plt.scatter(c_t[sel_t], r_t[sel_t], s=0.5, alpha=0.1)\n",
        "    plt.title(\"Test SPLADE 1% sparsity\")\n",
        "    save_fig(\"test_splade_pattern.png\")\n",
        "    plt.hist(c_t, bins=1000)\n",
        "    plt.title(\"Test Active Vocab IDs\")\n",
        "    save_fig(\"test_splade_vocab_hist.png\")\n",
        "    plt.plot(np.cumsum(svd.explained_variance_ratio_))\n",
        "    plt.title(\"Test Sparse SVD var\")\n",
        "    save_fig(\"test_svd_variance.png\")\n",
        "\n",
        "    plt.hist(cosims_test, bins=30)\n",
        "    plt.title(\"Test SBERT cosine sim\")\n",
        "    save_fig(\"test_cosine_sim.png\")\n",
        "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "    plt.title(\"Test PCA var\")\n",
        "    save_fig(\"test_pca_variance.png\")\n",
        "\n",
        "    for arr,name in [(tfidf_sim_test,\"tfidf\"), (jacc_test,\"jaccard\"), (lev_test,\"levenshtein\")]:\n",
        "        plt.hist(arr, bins=30)\n",
        "        plt.title(f\"Test {name} sim\")\n",
        "        save_fig(f\"test_{name}_hist.png\")\n",
        "\n",
        "    plt.hist(scores_test, bins=30)\n",
        "    plt.title(\"Test XE raw\")\n",
        "    save_fig(\"test_xe_raw.png\")\n",
        "    neg_t, pos_t = xe_feat_test[y_test==0], xe_feat_test[y_test==1]\n",
        "    plt.hist([neg_t,pos_t], bins=30, stacked=True)\n",
        "    plt.title(\"Test XE by label\")\n",
        "    save_fig(\"test_xe_by_label.png\")\n",
        "\n",
        "    return {\n",
        "        'stack_model':    stack,\n",
        "        'auc_xe_train':   auc_xe_train,\n",
        "        'auc_stack_train':auc_stack_train,\n",
        "        'auc_xe_test':    auc_xe_test,\n",
        "        'auc_stack_test': auc_stack_test,\n",
        "        'X_train':        X_train,\n",
        "        'X_test':         X_test\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ec9947",
      "metadata": {},
      "outputs": [],
      "source": [
        "trainNormalized.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2344fafd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from duplicate_question_pipeline import run_duplicate_pipeline\n",
        "\n",
        "df = trainNormalized.copy()\n",
        "# df = df.sample(n=5000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Full-text run\n",
        "X_full = df['question1'].astype(str).tolist()\n",
        "Y = df['is_duplicate'].values\n",
        "X2_full = df['question2'].astype(str).tolist()\n",
        "\n",
        "results_full = run_duplicate_pipeline(X_full, X2_full, Y, device='cuda', output_dir='results_full')\n",
        "\n",
        "# Token-based run\n",
        "tokens1 = df['tokens1'].apply(lambda t: ' '.join(t)).tolist()\n",
        "tokens2 = df['tokens2'].apply(lambda t: ' '.join(t)).tolist()\n",
        "results_tokens = run_duplicate_pipeline(tokens1, tokens2, Y, device='cuda', output_dir='results_tokens')\n",
        "\n",
        "# Noun-based run\n",
        "nouns1 = df['nouns1'].apply(lambda n: ' '.join(n)).tolist()\n",
        "nouns2 = df['nouns2'].apply(lambda n: ' '.join(n)).tolist()\n",
        "results_nouns = run_duplicate_pipeline(nouns1, nouns2, Y, device='cuda', output_dir='results_nouns')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "597dc764",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print function to display results of train and test AUCs\n",
        "def print_results(results, name):\n",
        "    print(f\"{name} Train AUC (XE): {results['auc_xe_train']:.4f}\")\n",
        "    print(f\"{name} Train AUC (Stack): {results['auc_stack_train']:.4f}\")\n",
        "    print(f\"{name} Test AUC (XE): {results['auc_xe_test']:.4f}\")\n",
        "    print(f\"{name} Test AUC (Stack): {results['auc_stack_test']:.4f}\")\n",
        "\n",
        "\n",
        "print_results(results_full, \"Full-text\")\n",
        "print_results(results_tokens, \"Tokens\")\n",
        "print_results(results_nouns, \"Nouns\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
