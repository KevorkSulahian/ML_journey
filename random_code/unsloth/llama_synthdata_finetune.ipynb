{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab2f5767",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm\n",
    "    !pip install synthetic-data-kit==0.0.3\n",
    "else:\n",
    "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
    "    !pip install --no-deps unsloth vllm==0.8.5.post1\n",
    "    !pip install synthetic-data-kit==0.0.3\n",
    "    !pip install transformers==4.51.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe9ceee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/herooooooooo/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 06-07 16:25:35 [__init__.py:243] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 16:25:36,402\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Using dtype = torch.bfloat16 for vLLM.\n",
      "Unsloth: vLLM loading unsloth/Llama-3.2-3B-Instruct with actual GPU utilization = 88.54%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 15.6 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 192.\n",
      "Unsloth: vLLM's KV Cache can use up to 7.83 GB. Also swap space = 2 GB.\n",
      "vLLM STDOUT: INFO 06-07 16:25:44 [__init__.py:243] Automatically detected platform cuda.\n",
      "vLLM STDOUT: INFO 06-07 16:25:46 [__init__.py:31] Available plugins for group vllm.general_plugins:\n",
      "vLLM STDOUT: INFO 06-07 16:25:46 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "vLLM STDOUT: INFO 06-07 16:25:46 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n",
      "vLLM STDOUT: INFO 06-07 16:25:47 [api_server.py:1289] vLLM API server version 0.9.0.1\n",
      "vLLM STDOUT: INFO 06-07 16:25:48 [cli_args.py:300] non-default args: {'dtype': 'bfloat16', 'seed': 0, 'max_model_len': 2048, 'max_logprobs': 0, 'gpu_memory_utilization': 0.8854324021821655, 'swap_space': 2.0, 'enable_prefix_caching': True, 'max_num_batched_tokens': 2048, 'max_num_seqs': 192, 'compilation_config': {\"level\": 3, \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}}, 'disable_log_stats': True}\n",
      "vLLM STDOUT: INFO 06-07 16:25:53 [config.py:793] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "vLLM STDOUT: INFO 06-07 16:25:53 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "vLLM STDOUT: INFO 06-07 16:25:56 [__init__.py:243] Automatically detected platform cuda.\n",
      "vLLM STDOUT: INFO 06-07 16:25:58 [core.py:438] Waiting for init message from front-end.\n",
      "vLLM STDOUT: INFO 06-07 16:25:58 [__init__.py:31] Available plugins for group vllm.general_plugins:\n",
      "vLLM STDOUT: INFO 06-07 16:25:58 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "vLLM STDOUT: INFO 06-07 16:25:58 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n",
      "vLLM STDOUT: INFO 06-07 16:25:58 [core.py:65] Initializing a V1 LLM engine (v0.9.0.1) with config: model='unsloth/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='unsloth/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 512}\n",
      "vLLM STDOUT: WARNING 06-07 16:25:58 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x76385f9f6b00>\n",
      "vLLM STDOUT: INFO 06-07 16:25:58 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "vLLM STDOUT: WARNING 06-07 16:25:58 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "vLLM STDOUT: INFO 06-07 16:25:58 [gpu_model_runner.py:1531] Starting to load model unsloth/Llama-3.2-3B-Instruct...\n",
      "vLLM STDOUT: INFO 06-07 16:25:58 [cuda.py:217] Using Flash Attention backend on V1 engine.\n",
      "vLLM STDOUT: INFO 06-07 16:25:58 [backends.py:35] Using InductorAdaptor\n",
      "vLLM STDOUT: INFO 06-07 16:25:59 [weight_utils.py:291] Using model weights format ['*.safetensors']\n",
      "vLLM STDOUT: INFO 06-07 16:28:03 [weight_utils.py:307] Time spent downloading weights for unsloth/Llama-3.2-3B-Instruct: 123.712223 seconds\n",
      "vLLM STDOUT: INFO 06-07 16:28:05 [default_loader.py:280] Loading weights took 1.16 seconds\n",
      "vLLM STDOUT: INFO 06-07 16:28:05 [gpu_model_runner.py:1549] Model loading took 6.0160 GiB and 126.156240 seconds\n",
      "vLLM STDOUT: INFO 06-07 16:28:08 [backends.py:459] Using cache directory: /home/herooooooooo/.cache/vllm/torch_compile_cache/1cc27d0e72/rank_0_0 for vLLM's torch.compile\n",
      "vLLM STDOUT: INFO 06-07 16:28:08 [backends.py:469] Dynamo bytecode transform time: 3.43 s\n",
      "vLLM STDOUT: INFO 06-07 16:28:10 [backends.py:158] Cache the graph of shape None for later use\n",
      "vLLM STDOUT: INFO 06-07 16:28:20 [backends.py:170] Compiling a graph for general shape takes 12.24 s\n",
      "vLLM STDOUT: INFO 06-07 16:28:28 [monitor.py:33] torch.compile takes 15.67 s in total\n",
      "vLLM STDOUT: INFO 06-07 16:28:29 [kv_cache_utils.py:637] GPU KV cache size: 59,088 tokens\n",
      "vLLM STDOUT: INFO 06-07 16:28:29 [kv_cache_utils.py:640] Maximum concurrency for 2,048 tokens per request: 28.85x\n",
      "vLLM STDOUT: INFO 06-07 16:28:46 [gpu_model_runner.py:1933] Graph capturing finished in 17 secs, took 0.45 GiB\n",
      "vLLM STDOUT: INFO 06-07 16:28:46 [core.py:167] init engine (profile, create kv cache, warmup model) took 41.61 seconds\n",
      "vLLM STDOUT: WARNING 06-07 16:28:47 [config.py:1339] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
      "vLLM STDOUT: INFO 06-07 16:28:47 [serving_chat.py:117] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
      "vLLM STDOUT: INFO 06-07 16:28:47 [serving_completion.py:65] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
      "vLLM STDOUT: INFO 06-07 16:28:47 [api_server.py:1336] Starting vLLM API server on http://0.0.0.0:8000\n",
      "\n",
      "--- vLLM Server Ready (Detected: 'Starting vLLM API server on') ---\n"
     ]
    }
   ],
   "source": [
    "from unsloth.dataprep import SyntheticDataKit\n",
    "\n",
    "generator = SyntheticDataKit.from_pretrained(\n",
    "    # Choose any model from https://huggingface.co/unsloth\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = 2048, # Longer sequence lengths will be slower!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d482e94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.prepare_qa_generation(\n",
    "    output_folder = \"data\", # Output location of synthetic data\n",
    "    temperature = 0.7, # Higher temp makes more diverse datases\n",
    "    top_p = 0.95,\n",
    "    overlap = 64, # Overlap portion during chunking\n",
    "    max_generation_tokens = 512, # Can increase for longer QA pairs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c7be1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[32m VLLM server is running at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
      "\u001b[2KAvailable models: \u001b[1m{\u001b[0m\u001b[32m'object'\u001b[0m: \u001b[32m'list'\u001b[0m, \u001b[32m'data'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
      "\u001b[32m'unsloth/Llama-3.2-3B-Instruct'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model'\u001b[0m, \u001b[32m'created'\u001b[0m: \u001b[1;36m1749300234\u001b[0m, \n",
      "\u001b[32m'owned_by'\u001b[0m: \u001b[32m'vllm'\u001b[0m, \u001b[32m'root'\u001b[0m: \u001b[32m'unsloth/Llama-3.2-3B-Instruct'\u001b[0m, \u001b[32m'parent'\u001b[0m: \u001b[3;35mNone\u001b[0m, \n",
      "\u001b[32m'max_model_len'\u001b[0m: \u001b[1;36m2048\u001b[0m, \u001b[32m'permission'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
      "\u001b[32m'modelperm-d3551765d5494e4386b5ba99dbfe397c'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model_permission'\u001b[0m, \n",
      "\u001b[32m'created'\u001b[0m: \u001b[1;36m1749300234\u001b[0m, \u001b[32m'allow_create_engine'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_sampling'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
      "\u001b[32m'allow_logprobs'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'allow_search_indices'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_view'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
      "\u001b[32m'allow_fine_tuning'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'organization'\u001b[0m: \u001b[32m'*'\u001b[0m, \u001b[32m'group'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'is_blocking'\u001b[0m: \n",
      "\u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
      "\u001b[2K\u001b[32mâ ‹\u001b[0m Checking VLLM server at http://localhost:8000/v1...\n",
      "\u001b[1A\u001b[2K"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit system-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de5f3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[32mâ ¦\u001b[0m Processing https://arxiv.org/html/2412.09871v1.....\n",
      "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/arxiv_org.txt\u001b[0m\n",
      "37 ['data/output/arxiv_org_0.txt', 'data/output/arxiv_org_1.txt', 'data/output/arxiv_org_2.txt']\n"
     ]
    }
   ],
   "source": [
    "# Byte Latent Transformer: Patches Scale Better Than Tokens paper in HTML format\n",
    "!synthetic-data-kit \\\n",
    "    -c synthetic_data_kit_config.yaml \\\n",
    "    ingest \"https://arxiv.org/html/2412.09871v1\"\n",
    "\n",
    "# Truncate document\n",
    "filenames = generator.chunk_data(\"data/output/arxiv_org.txt\")\n",
    "print(len(filenames), filenames[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0847abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2KProcessing 1 chunks to generate QA pairs...output/arxiv_org_0.txt.....\n",
      "\u001b[2KBatch processing complete.ontent from data/output/arxiv_org_0.txt...\n",
      "\u001b[2KGenerated 9 QA pairs totalnt from data/output/arxiv_org_0.txt...\n",
      "\u001b[2KSaving result to data/generated/arxiv_org_0_qa_pairs.json.txt...\n",
      "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json..\n",
      "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_0_qa_pairs.json\n",
      "\u001b[2K\u001b[32mâ ‹\u001b[0m Generating qa content from data/output/arxiv_org_0.txt...\n",
      "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_0_qa_pairs.json\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2KProcessing 1 chunks to generate QA pairs...output/arxiv_org_1.txt.....\n",
      "\u001b[2KBatch processing complete.ontent from data/output/arxiv_org_1.txt...\n",
      "\u001b[2KGenerated 13 QA pairs totalt from data/output/arxiv_org_1.txt...\n",
      "\u001b[2KSaving result to data/generated/arxiv_org_1_qa_pairs.json.txt...\n",
      "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json..\n",
      "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_1_qa_pairs.json\n",
      "\u001b[2K\u001b[32mâ ¹\u001b[0m Generating qa content from data/output/arxiv_org_1.txt...\n",
      "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_1_qa_pairs.json\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2KProcessing 1 chunks to generate QA pairs...output/arxiv_org_2.txt.....\n",
      "\u001b[2KBatch processing complete.ontent from data/output/arxiv_org_2.txt...\n",
      "\u001b[2KGenerated 11 QA pairs totalt from data/output/arxiv_org_2.txt...\n",
      "\u001b[2KSaving result to data/generated/arxiv_org_2_qa_pairs.json.txt...\n",
      "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json..\n",
      "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_2_qa_pairs.json\n",
      "\u001b[2K\u001b[32mâ ¸\u001b[0m Generating qa content from data/output/arxiv_org_2.txt...\n",
      "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_2_qa_pairs.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Process 3 chunks for now -> can increase but slower!\n",
    "for filename in filenames[:3]:\n",
    "    !synthetic-data-kit \\\n",
    "        -c synthetic_data_kit_config.yaml \\\n",
    "        create {filename} \\\n",
    "        --num-pairs 25 \\\n",
    "        --type \"qa\"\n",
    "    time.sleep(2) # Sleep some time to leave some room for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4afe702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lProcessing 2 batches of QA pairs...\n",
      "\u001b[2KBatch processing complete.t from data/generated/arxiv_org_0_qa_pairs.json...\n",
      "\u001b[2KRated 9 QA pairscontent from data/generated/arxiv_org_0_qa_pairs.json...\n",
      "\u001b[2KRetained 9 pairs (threshold: 0.0)/generated/arxiv_org_0_qa_pairs.json...\n",
      "\u001b[2KAverage score: 8.3ntent from data/generated/arxiv_org_0_qa_pairs.json...\n",
      "\u001b[2K\u001b[32mâ ´\u001b[0m Cleaning content from data/generated/arxiv_org_0_qa_pairs.json...\n",
      "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/arxiv_org_0_qa_pairs_cleaned.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit \\\n",
    "    -c synthetic_data_kit_config.yaml \\\n",
    "    curate --threshold 0.0 \\\n",
    "    \"data/generated/arxiv_org_0_qa_pairs.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "388aa0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[32mâ ‹\u001b[0m Converting data/generated/arxiv_org_0_qa_pairs.json to ft format with json \n",
      "storage...\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/arxiv_org_0_qa_pairs_ft.json\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[32mâ ‹\u001b[0m Converting data/generated/arxiv_org_1_qa_pairs.json to ft format with json \n",
      "storage...\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/arxiv_org_1_qa_pairs_ft.json\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[32mâ ‹\u001b[0m Converting data/generated/arxiv_org_2_qa_pairs.json to ft format with json \n",
      "storage...\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/arxiv_org_2_qa_pairs_ft.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "qa_pairs_filenames = [\n",
    "    f\"data/generated/arxiv_org_{i}_qa_pairs.json\"\n",
    "    for i in range(len(filenames[:3]))\n",
    "]\n",
    "for filename in qa_pairs_filenames:\n",
    "    !synthetic-data-kit \\\n",
    "        -c synthetic_data_kit_config.yaml \\\n",
    "        save-as {filename} -f ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "501c1531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "final_filenames = [\n",
    "    f\"data/final/arxiv_org_{i}_qa_pairs_ft.json\"\n",
    "    for i in range(len(filenames[:3]))\n",
    "]\n",
    "conversations = pd.concat([\n",
    "    pd.read_json(name) for name in final_filenames\n",
    "]).reset_index(drop = True)\n",
    "\n",
    "dataset = Dataset.from_pandas(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1729d120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'},\n",
       "  {'content': 'What is the main innovation of the Byte Latent Transformer (BLT) architecture?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation.',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af8ea988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to terminate the VLLM server gracefully...\n",
      "Server terminated gracefully.\n"
     ]
    }
   ],
   "source": [
    "generator.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91866f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.1: Fast Llama patching. Transformers: 4.52.4. vLLM: 0.9.0.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090 Laptop GPU. Num GPUs = 1. Max memory: 15.596 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "    # Qwen3 new models\n",
    "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    # Other very popular models!\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/Llama-3.3-70B\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
    "    \"unsloth/Phi-4\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = 2048, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a6c28bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.6.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafb6b2b",
   "metadata": {},
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the `Llama-3.2` format for conversation style finetunes. The chat template renders conversations like below: (Cutting Knowledge Date is by default there!)\n",
    "\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 01 May 2025\n",
    "\n",
    "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "What is 1+1?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "2<|eot_id|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d2e9c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 2522.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "# Get our previous dataset and format it:\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dc793ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'},\n",
       "  {'content': 'What is the main innovation of the Byte Latent Transformer (BLT) architecture?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation.',\n",
       "   'role': 'assistant'}],\n",
       " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 07 Jun 2025\\n\\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the main innovation of the Byte Latent Transformer (BLT) architecture?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nBLT encodes bytes into dynamically sized patches, which serve as the primary units of computation.<|eot_id|>'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8a37af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:03<00:00,  8.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff014f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090 Laptop GPU. Max memory = 15.596 GB.\n",
      "3.441 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "847d4096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 33 | Num Epochs = 12 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 24,313,856/3,000,000,000 (0.81% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:25, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.531500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.660700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.772200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.385400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.152700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.699600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.025400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.874200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.722400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.865200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.258100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.879800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.137400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.858600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.672600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.707700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.618500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.599800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.592900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.580400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.288200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.258600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.342300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.184500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.220400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.343400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.077900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.045200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.013100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.679700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.998900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.772400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.830400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.917700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.799400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.693500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.722200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.750700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.797300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.601800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.673900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.650700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.432300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.505600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.637200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.518100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.765400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.490500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.492500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.428800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.561100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.608000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89464229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLT is a lightweight architecture that combines the efficiency of byte tokenization with the scaling capabilities of transformer models.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the Byte Latent Transformer?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer,\n",
    "                   max_new_tokens = 256, temperature = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5398403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BLT provides improvements in both training and inference efficiency while maintaining a comparable number of floating-point operations.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What are some benefits of the BLT?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer,\n",
    "                   max_new_tokens = 256, temperature = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5373462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/herooooooooo/llama_finetuned_lora\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub(\"herooooooooo/llama_finetuned_lora\", token = \"...\") # Online saving\n",
    "tokenizer.push_to_hub(\"herooooooooo/llama_finetuned_lora\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ebe8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
