{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not going to do full attention explanation, just sharing the notes that I think are important. Especially for me to remember for my tasks.\n",
    "\n",
    "\n",
    "Attention works by creating query \\(Q\\), key \\(K\\), and value \\(V\\) matrices from inputs \\(X\\) via linear layers with learnable weights \\(W_Q\\), \\(W_K\\), and \\(W_V\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q = XW^Q \\\\\n",
    "K = XW^K \\\\\n",
    "V = XW^V\n",
    "$$\n",
    "\n",
    "\n",
    "![Self-Attention Matrix Calculation](https://benjaminwarner.dev/img/2022/tinkering-with-attention-pooling/self-attention-matrix-calculation-queries.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $ W^Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_Q} $. Or less formally, $ Q = XW^Q $ is a set of linear equations:\n",
    "\n",
    "$\n",
    "Q = X A^Q + B^Q\n",
    "$\n",
    "\n",
    "where $ A^Q $ and $ B^Q $ are learnable parameters for calculating $ Q $ from $ X $.\n",
    "\n",
    "Attention is then calculated by:\n",
    "\n",
    "$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$\n",
    "\n",
    "where $ \\sqrt{d_k} $ is a scaling factor, usually based on the individual head dimension or number of heads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Self-Attention Calculation](https://benjaminwarner.dev/img/2022/tinkering-with-attention-pooling/self-attention-matrix-calculation-attention.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting $ \\text{Attention}(Q, K, V) $ is usually passed through a linear layer $ W^O $ projection\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{Attention}(Q, K, V) W^O\n",
    "$$\n",
    "\n",
    "as the final step of the Attention layer.\n",
    "\n",
    "For all the math, Attention is simply a learned weighted average. Attention learns to generate weights between tokens via queries $ XW^Q $ and keys $ XW^K $. Those per-token weights are created by $ \\text{softmax}(QK^T / \\sqrt{d_k}) $. The values $ XW^V $ learn to create a token representation which can incorporate the weighted average of all the other tokens in the final dot product in the Attention layer $ \\text{softmax}(\\dots) V $. When someone says a token attends to a second token, this means it’s increasing the size of the second token’s weight in the weighted average relative to all the other tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single headed attention initialization\n",
    "Attention layers will allow disabling bias terms for linear layers since recent papers and models, such as Cramming, Pythia, and PaLM, have shown that disabling the bias term results in little-to-no downstream performance drop\n",
    "In a well-trained NLP Transformer, such as Pythia, the bias term ends up being near or at zero, which is why we can disable them without causing performance issues.\n",
    "while decreasing computational and memory requirements.\n",
    "\n",
    "```python\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "        hidden_size: int,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "```\n",
    "\n",
    "It's possible to merge $W^Q$ and $W^K$ into a single matrix $W^{QK}$ and $W^V$ into a single matrix $W_qkv$ and then $unbind$ them into $W^Q$ and $W^K$ and $W^V$ matrices.\n",
    "In Multi-Head Attention, each individual head size is smaller than the input size, so for Single Head we will arbitrarily set the head size to be four times smaller than the input dimension.\n",
    "```python\n",
    "# linear layer to project queries, keys, values\n",
    "Wqkv = nn.Linear(hidden_size, (hidden_size//4)*3, bias=bias)\n",
    "# linear layer to project final output\n",
    "proj = nn.Linear(hidden_size//4, hidden_size, bias=bias)\n",
    "```\n",
    "\n",
    "And that’s it for the Attention initialization. The Attention mechanism in a Transformer only has two layers of learnable parameters. Everything else in Attention is an operation on the output of the Wqkv linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Head Forward\n",
    "\n",
    "After some input shape housekeeping, the first computational step is to generate our keys, queries, and values. First, we pass the input $x$ through the $Wqkv$.\n",
    "Then we reshape the $Wqkv$ output to batch size, sequence length, one dimension for $Q K V$, and the head size.\n",
    "Finally, we split the single tensor into the query, key, and value tensors using unbind, where each are of shape B, S, C//4.\n",
    "```python\n",
    "# batch size (B), sequence length (S), input dimension (C)\n",
    "B, S, C = x.shape\n",
    "\n",
    "# split into queries, keys, & values of shape\n",
    "# batch size (B), sequence length (S), head size (HS)\n",
    "q, k, v = self.Wqkv(x).reshape(B, S, 3, C//4).unbind(dim=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the queries, keys, and values generated, we can move to the mathematical operations of the Attention mechanism.\n",
    "\n",
    "So first, we need to transpose $K$ and take the dot product of $Q$ and $K^T$.\n",
    "\n",
    "```python\n",
    "# calculate dot product of queries and keys of shape\n",
    "# (B, S, S) = (B, S, HS) @ (B, HS, S)\n",
    "attn = q @ k.transpose(-2, -1)\n",
    "```\n",
    "\n",
    "Next, we need to scale the outputs of the $QK^T$ by $\\sqrt{d_k}$.\n",
    "\n",
    "```python\n",
    "# scale by square root of head dimension\n",
    "attn = attn / math.sqrt(k.size(-1))\n",
    "```\n",
    "\n",
    "it’s time to calculate the token Attention weight using softmax.\n",
    "\n",
    "```python\n",
    "# apply softmax to get attention weights\n",
    "attn = attn.softmax(dim=-1)\n",
    "```\n",
    "\n",
    "This Softmax output of $QK^T/ \\sqrt{d_k}$ is how the Attention mechanism weights the strength of the relationship between each pair of tokens. Where higher Softmax values means Attention is placing more importance on these pairs of tokens and lower values are deemed less important.\n",
    "\n",
    "Next we matrix multiply the Attention weights with our value matrix $V$ which applies the Attention weights to our propagating token embeddings\n",
    "\n",
    "```python\n",
    "# dot product attention weights to values\n",
    "# (B, S, HS) = (B, S, S) @ (B, S, HS)\n",
    "x = attn @ v\n",
    "```\n",
    "\n",
    "Finally, we project the output of the Attention mechanism back to the original input dimension using the $proj$ linear layer.\n",
    "\n",
    "```python\n",
    "# project back to original dimension\n",
    "x = self.proj(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it. A simple rendition of Single Head Bidirectional Attention in code.\n",
    "\n",
    "```python\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "        hidden_size: int,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.Wqkv = nn.Linear(hidden_size, (hidden_size//4)*3, bias=bias)\n",
    "        self.Wo = nn.Linear(hidden_size//4, hidden_size, bias=bias)\n",
    "\n",
    "    def forward(self, x:Tensor):\n",
    "        B, S, C = x.shape\n",
    "\n",
    "        q, k, v = self.Wqkv(x).reshape(B, S, 3, C//4).unbind(dim=2)\n",
    "\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        attn = attn / math.sqrt(k.size(-1))\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        x = attn @ v\n",
    "\n",
    "        return self.Wo(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Self-Attention\n",
    "\n",
    "Formally, Multi-Head Attention creates one query $ Q_h $, key $ K_h $, and value $ V_h $ per head $ h $, calculates the scaled dot-product Attention per head $ \\text{Attention}(Q_h, K_h, V_h) $, concatenates all the Attention outputs back into one tensor $ \\text{MultiHead}(Q, K, V) $, before passing the Multi-Head Attention output through the final linear layer $ W^O $:\n",
    "\n",
    "$$\n",
    "Q_h = X W_h^Q \\quad K_h = X W_h^K \\quad V_h = X W_h^V\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q_h, K_h, V_h) = \\text{softmax} \\left( \\frac{Q_h K_h^T}{\\sqrt{d_h}} \\right) V_h\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{concat}(\\text{Attention}(Q_h, K_h, V_h), \\text{ for all } h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{MultiHead}(Q, K, V) W^O\n",
    "$$\n",
    "\n",
    "\n",
    "```python\n",
    "def __init__(self,\n",
    "    hidden_size: int,\n",
    "    num_heads: int,\n",
    "    bias: bool = True,\n",
    "):\n",
    "    # input dimension must be divisible by num_heads\n",
    "    assert hidden_size % num_heads == 0\n",
    "    # number of attention heads\n",
    "    self.nh = num_heads\n",
    "    super().__init__()\n",
    "    # linear layer to project queries, keys, values\n",
    "    self.Wqkv = nn.Linear(hidden_size, hidden_size*3, bias=bias)\n",
    "    # linear layer to project final output\n",
    "    self.Wo = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "```\n",
    "\n",
    "\n",
    "Our Multi-Head forward method is largely the same, with a few changes to account for the multiple heads.\n",
    "\n",
    "Our input sequence is projected through the linear Wqkv layer as before. Then we need to reshape and transpose the output to batch size, number of heads, \n",
    "$Q_h$ $K_h$ $V_h$ sequence length, and the head size, which in most Transformers is the embedding shape divided by the number of heads. Then we unbind our reshaped and transposed output to the separate queries, keys, & values, each of shape B, NH, S, HS.\n",
    "```python\n",
    "# batch size (B), sequence length (S), input dimension (C)\n",
    "B, S, C = x.shape\n",
    "\n",
    "# split into queries, keys, & values of shape\n",
    "# batch size (B), num_heads (NH), sequence length (S), head size (HS)\n",
    "x = self.Wqkv(x).reshape(B, S, 3, self.nh, C//self.nh)\n",
    "q, k, v = x.transpose(3, 1).unbind(dim=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Attention mechanism is exactly the same as the Single Head code, but the difference in tensor shape means we are calculating the Softmax individually per each head\n",
    "\n",
    "```python\n",
    "# calculate dot product of queries and keys\n",
    "# (B, NH, S, S) = (B, NH, S, HS) @ (B, NH, HS, S)\n",
    "attn = q @ k.transpose(-2, -1)\n",
    "\n",
    "# scale by square root of head dimension\n",
    "attn = attn / math.sqrt(k.size(-1))\n",
    "\n",
    "# apply softmax to get attention weights\n",
    "attn = attn.softmax(dim=-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our remaining steps are to matrix multiply the Attention outputs with $V_h$ , then concatenate the per-head Attention into one output of our input shape.\n",
    "\n",
    "We perform this by transposing the heads and sequences then reshaping to B, S, C. This is mechanically the same as a concatenation, without the requirement of creating a new tensor.\n",
    "\n",
    "```python\n",
    "\n",
    "# dot product attention weights with values\n",
    "# (B, NH, S, HS) = (B, NH, S, S) @ (B, NH, HS, S)\n",
    "x = attn @ v\n",
    "\n",
    "# transpose heads & sequence then reshape back to (B, S, C)\n",
    "x = x.transpose(1, 2).reshape(B, S, C)\n",
    "\n",
    "# apply final linear layer to get output\n",
    "return self.Wo(x)\n",
    "\n",
    "```\n",
    "\n",
    "With all the pieces defined, we now have a working, albeit incomplete, implementation of Bidirectional Self-Attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "        hidden_size: int,\n",
    "        num_heads: int,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0\n",
    "        self.nh = num_heads\n",
    "        self.Wqkv = nn.Linear(hidden_size, hidden_size * 3, bias=bias)\n",
    "        self.Wo = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        B, S, C = x.shape\n",
    "\n",
    "        x = self.Wqkv(x).reshape(B, S, 3, self.nh, C//self.nh)\n",
    "        q, k, v = x.transpose(3, 1).unbind(dim=2)\n",
    "\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        attn = attn / math.sqrt(k.size(-1))\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        x = attn @ v\n",
    "\n",
    "        return self.Wo(x.transpose(1, 2).reshape(B, S, C))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Self-Attention\n",
    "\n",
    "```python\n",
    "# causal mask to ensure that attention is not applied to future tokens\n",
    "# where context_size is the maximum sequence length of the transformer\n",
    "self.register_buffer('causal_mask',\n",
    "    torch.triu(torch.ones([context_size, context_size],\n",
    "               dtype=torch.bool), diagonal=1)\n",
    "        .view(1, 1, context_size, context_size))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then in our CausalAttention forward method, we use masked_fill again to apply the causal mask to our intermediate Attention results before applying softmax to calculate the Attention weights.\n",
    "\n",
    "```python\n",
    "# scale by square root of output dimension\n",
    "attn = attn / math.sqrt(k.size(-1))\n",
    "\n",
    "# apply causal mask\n",
    "attn = attn.masked_fill(self.causal_mask[:, :, :S, :S], float('-inf'))\n",
    "\n",
    "# apply softmax to get attention weights\n",
    "attn = attn.softmax(dim=-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that’s it! We now have a working implementation of Causal Self-Attention.\n",
    "\n",
    "```python\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "        hidden_size: int,\n",
    "        num_heads: int,\n",
    "        context_size: int,\n",
    "        attn_drop: float = 0.1,\n",
    "        out_drop: float = 0.1,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0\n",
    "        self.nh = num_heads\n",
    "        self.Wqkv = nn.Linear(hidden_size, hidden_size * 3, bias=bias)\n",
    "        self.Wo = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.out_drop = nn.Dropout(out_drop)\n",
    "        self.register_buffer('causal_mask',\n",
    "            torch.triu(torch.ones([context_size, context_size],\n",
    "                       dtype=torch.bool), diagonal=1)\n",
    "                .view(1, 1, context_size, context_size))\n",
    "\n",
    "    def forward(self, x: Tensor, mask: BoolTensor):\n",
    "        B, S, C = x.shape\n",
    "\n",
    "        x = self.Wqkv(x).reshape(B, S, 3, self.nh, C//self.nh)\n",
    "        q, k, v = x.transpose(3, 1).unbind(dim=2)\n",
    "\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        attn = attn / math.sqrt(k.size(-1))\n",
    "\n",
    "        combined_mask = self.causal_mask[:, :, :S, :S] + mask.view(B, 1, 1, S)\n",
    "        attn = attn.masked_fill(combined_mask, float('-inf'))\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = attn @ v\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, S, C)\n",
    "        return self.out_drop(self.Wo(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
