{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08d5520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file collects all the relevant code that we covered thus far\n",
    "# throughout Chapters 3-4.\n",
    "# This file can be run as a standalone script.\n",
    "\n",
    "import time\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 3\n",
    "#####################################\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False, max_seq_len=None, window_size=None):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        ####################################################\n",
    "        # NEW\n",
    "        self.max_seq_len = max_seq_len or context_length\n",
    "        self.window_size = window_size or self.max_seq_len\n",
    "        self.register_buffer(\"cache_k\", None, persistent=False)\n",
    "        self.register_buffer(\"cache_v\", None, persistent=False)\n",
    "        ####################################################\n",
    "\n",
    "    def forward(self, x, use_cache=False):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys_new = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        values_new = self.W_value(x)\n",
    "        queries = self.W_query(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys_new = keys_new.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values_new = values_new.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys_new = keys_new.transpose(1, 2)\n",
    "        values_new = values_new.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "\n",
    "        ####################################################\n",
    "        # NEW\n",
    "        if use_cache:\n",
    "            if self.cache_k is None or self.cache_k.size(0) != b:\n",
    "                self.cache_k = torch.zeros(b, self.num_heads, self.max_seq_len, self.head_dim, device=x.device)\n",
    "                self.cache_v = torch.zeros(b, self.num_heads, self.max_seq_len, self.head_dim, device=x.device)\n",
    "                self.current_pos = 0\n",
    "\n",
    "            # write new entries\n",
    "            start = self.current_pos\n",
    "            end = start + num_tokens\n",
    "            self.cache_k[:, :, start:end, :] = keys_new\n",
    "            self.cache_v[:, :, start:end, :] = values_new\n",
    "            self.current_pos = end\n",
    "\n",
    "            # sliding window truncation\n",
    "            if self.current_pos > self.window_size:\n",
    "                self.cache_k = self.cache_k[:, :, -self.window_size:, :]\n",
    "                self.cache_v = self.cache_v[:, :, -self.window_size:, :]\n",
    "                self.current_pos = self.window_size\n",
    "\n",
    "            keys = self.cache_k[:, :, :self.current_pos, :]\n",
    "            values = self.cache_v[:, :, :self.current_pos, :]\n",
    "        else:\n",
    "            keys = keys_new\n",
    "            values = values_new\n",
    "        ####################################################\n",
    "\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        ####################################################\n",
    "        # NEW\n",
    "        K = attn_scores.size(-1)\n",
    "\n",
    "        if num_tokens == K:\n",
    "            # No cache → use the pre‑baked triangular mask slice\n",
    "            causal_mask = torch.triu(torch.ones(num_tokens, K, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        else:\n",
    "            # Cached: need to offset the diagonal by (K − num_tokens)\n",
    "            offset = K - num_tokens  # number of tokens already in cache before this chunk\n",
    "            row_idx = torch.arange(num_tokens, device=x.device).unsqueeze(1)  # (num_tokens, 1)\n",
    "            col_idx = torch.arange(K, device=x.device).unsqueeze(0)           # (1, K)\n",
    "            causal_mask = row_idx + offset < col_idx                          # True where j > i+offset\n",
    "        ####################################################\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(causal_mask.unsqueeze(0).unsqueeze(0), -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "    ####################################################\n",
    "    # NEW\n",
    "    def reset_cache(self):\n",
    "        self.cache_k, self.cache_v = None, None\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 4\n",
    "#####################################\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"],\n",
    "            window_size=cfg[\"kv_window_size\"])  # NEW\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x, use_cache=False):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        ####################################################\n",
    "        # NEW\n",
    "        x = self.att(x, use_cache=use_cache)\n",
    "        ####################################################\n",
    "\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # self.trf_blocks = nn.Sequential(\n",
    "        #    *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        ####################################################\n",
    "        # NEW\n",
    "        self.trf_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.current_pos = 0\n",
    "        ####################################################\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx, use_cache=False):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "\n",
    "        # pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "\n",
    "        ####################################################\n",
    "        # NEW\n",
    "\n",
    "        if use_cache:\n",
    "            pos_ids = torch.arange(self.current_pos, self.current_pos + seq_len, device=in_idx.device, dtype=torch.long)\n",
    "            self.current_pos += seq_len\n",
    "        else:\n",
    "            pos_ids = torch.arange(0, seq_len, device=in_idx.device, dtype=torch.long)\n",
    "        pos_embeds = self.pos_emb(pos_ids).unsqueeze(0)\n",
    "        ####################################################\n",
    "\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        # x = self.trf_blocks(x)\n",
    "        ####################################################\n",
    "        # NEW\n",
    "        for blk in self.trf_blocks:\n",
    "            x = blk(x, use_cache=use_cache)\n",
    "        ####################################################\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "    ####################################################\n",
    "    # NEW\n",
    "    def reset_kv_cache(self):\n",
    "        for blk in self.trf_blocks:\n",
    "            blk.att.reset_cache()\n",
    "\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "####################################################\n",
    "# NEW\n",
    "def generate_text_simple_cached(model, idx, max_new_tokens):\n",
    "    model.eval()\n",
    "    model.reset_kv_cache()\n",
    "\n",
    "    # Init cache with full prompt\n",
    "    logits = model(idx, use_cache=True)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        last_logits = logits[:, -1]\n",
    "        next_idx = last_logits.argmax(dim=-1, keepdim=True)\n",
    "        idx = torch.cat([idx, next_idx], dim=1)\n",
    "\n",
    "        logits = model(next_idx, use_cache=True)\n",
    "\n",
    "    return idx\n",
    "####################################################\n",
    "\n",
    "\n",
    "def main():\n",
    "    GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,     # Vocabulary size\n",
    "        \"context_length\": 1024,  # Context length\n",
    "        \"emb_dim\": 768,          # Embedding dimension\n",
    "        \"n_heads\": 12,           # Number of attention heads\n",
    "        \"n_layers\": 12,          # Number of layers\n",
    "        \"drop_rate\": 0.1,        # Dropout rate\n",
    "        \"qkv_bias\": False,       # Query-Key-Value bias\n",
    "        \"kv_window_size\": 48     # NEW: KV cache window size\n",
    "    }\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # disable dropout\n",
    "\n",
    "    start_context = \"Hello, I am\"\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    encoded = tokenizer.encode(start_context)\n",
    "    encoded_tensor = torch.tensor(encoded, device=device).unsqueeze(0)\n",
    "\n",
    "    print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
    "    print(\"\\nInput text:\", start_context)\n",
    "    print(\"Encoded input text:\", encoded)\n",
    "    print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "\n",
    "    # token_ids = generate_text_simple(\n",
    "    #     model=model,\n",
    "    #     idx=encoded_tensor,\n",
    "    #     max_new_tokens=200,\n",
    "    #     context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    "    # )\n",
    "\n",
    "    ####################################################\n",
    "    # NEW\n",
    "    token_ids = generate_text_simple_cached(\n",
    "        model=model,\n",
    "        idx=encoded_tensor,\n",
    "        max_new_tokens=200,\n",
    "    )\n",
    "    ####################################################\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    total_time = time.time() - start\n",
    "\n",
    "    decoded_text = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "\n",
    "    print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n",
    "    print(\"\\nOutput:\", token_ids)\n",
    "    print(\"Output length:\", len(token_ids[0]))\n",
    "    print(\"Output text:\", decoded_text)\n",
    "\n",
    "    print(f\"\\nTime: {total_time:.2f} sec\")\n",
    "    print(f\"{int(len(token_ids[0])/total_time)} tokens/sec\")\n",
    "    if torch.cuda.is_available():\n",
    "        max_mem_bytes = torch.cuda.max_memory_allocated()\n",
    "        max_mem_gb = max_mem_bytes / (1024 ** 3)\n",
    "        print(f\"Max memory allocated: {max_mem_gb:.2f} GB\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21c661e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: Hello, I am\n",
      "Encoded input text: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      OUT\n",
      "==================================================\n",
      "\n",
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267,\n",
      "         49706, 43231, 47062, 34657, 18631, 49188, 43312, 45933, 23154, 15983,\n",
      "         10345, 16369, 46214, 22954, 34674, 21100,  4743, 14056, 42526,  6459,\n",
      "         12799,  5734, 49274,   136, 49294, 42900, 21193, 20463,  1018,  7864,\n",
      "         13895, 27167, 12810, 25727, 14388,   985, 15797, 24440, 18557, 31346,\n",
      "         38621, 48277,  8181, 34119,  3842, 49622, 18465,  6463, 39047, 21487,\n",
      "          9117, 41128, 41886, 37493, 10241,  1060, 39417, 29209, 38328, 34379,\n",
      "         19619, 24851, 14182, 42633, 48956, 49487, 29773,   393,  4247, 18992,\n",
      "         45900, 22015, 26541,  3333, 40681, 38073, 36225, 44351, 29739, 19480,\n",
      "          5560, 41505, 19372, 22934, 16434, 41680,  9277, 22282, 36191, 44269,\n",
      "         17284, 37646, 32837,  1557, 39364, 31114, 11983,  5248, 35360, 40259,\n",
      "         17185, 21275, 28388, 19086, 15495, 40425, 40006, 33058,  9033,  6746,\n",
      "         22923, 48762, 10335,  8549, 31848,  6327, 21171, 18551, 38996, 36750,\n",
      "         34677, 33980, 41537,  1872,  2156, 23983,  4896, 45313, 20883, 13590,\n",
      "         32132,  6450,  4333, 36408, 15648, 27025,  8393, 13623,  9528, 15723,\n",
      "         20884, 26190, 14799, 36373, 15724, 35331, 49956, 25735, 39991,   139,\n",
      "          7520, 15637, 13342, 27855, 32951, 32256, 33932,  1380,  5683, 37110,\n",
      "          8894, 31058,  2843, 36226, 25572, 49297, 37278, 46056, 29747, 16496,\n",
      "         26394, 38055, 10745, 27099, 36825, 33282, 14112, 33549, 30441, 49812,\n",
      "         35484, 28117,  4851, 17249, 17027, 17533, 14407, 25401,  2359, 18830,\n",
      "         38338, 42289, 38956,  9491]], device='cuda:0')\n",
      "Output length: 204\n",
      "Output text: Hello, I am Featureiman Byeswickattribute argue logger Normandy Compton analogous bore ITVEGIN ministriesysics Kle functional recountrictionchangingVirgin embarrassedgl Truthfoundation challenges essence specifically Absent� 421 lov Castro Fewug wins Hus Golemllyyll Fisher sim trimュPrint Saskatchewanarynerrors hang lows activity litresNothing cannhound utilized extend eerie Emerson bursting pregnancyirect invaders surrenderedRossWriterCalifetti glucHurSCPammu� orasion improper 414 arous Sergeantirth racists497ahah despise Exit Fri pil Petty Swe tyr renewed besieged arbitilib Scor Curious Valent arteryaunaiqu sizeof Become participatingSpe 254browser disappointing intox blessings priorit 108 Reserv Twisted Thoughts Ze SaudCBS Valhalla Prom Management Seventhmond Adult CF Fasterlocalhost HIGH evilsmonthsai house Testing investmentmini lining Shot yogurt sand Poweriannopoulos Rise Frances Exec rats clothing Arctic freezing Sebastian independently twisting2004 Sophiefrog systematically shards� granted limiting globe capsule revitalbike cavern Ph Ext falsehood Heartierre Mich mL lunar wetlands needy fearsome Dollarzilla bean 8000inf deriveawedbrown infected synth scrutin aph bananaseenthFClatedzi bored Phantom OTHERduct sanction eatergorithm968jar\n",
      "\n",
      "Time: 1.32 sec\n",
      "154 tokens/sec\n",
      "Max memory allocated: 0.89 GB\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cfedd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
