{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytearray(b'This is some text')\n"
     ]
    }
   ],
   "source": [
    "text = \"This is some text\"\n",
    "byte_array = bytearray(text, 'utf-8')\n",
    "print(byte_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 104, 105, 115, 32, 105, 115, 32, 115, 111, 109, 101, 32, 116, 101, 120, 116]\n"
     ]
    }
   ],
   "source": [
    "ids = list(byte_array)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212, 318, 617, 2420]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt2_tokenizer.encode(\"This is some text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: !\n",
      "1: \"\n",
      "2: #\n",
      "3: $\n",
      "4: %\n",
      "5: &\n",
      "6: '\n",
      "7: (\n",
      "8: )\n",
      "9: *\n",
      "10: +\n",
      "11: ,\n",
      "12: -\n",
      "13: .\n",
      "14: /\n",
      "15: 0\n",
      "16: 1\n",
      "17: 2\n",
      "18: 3\n",
      "19: 4\n",
      "20: 5\n",
      "21: 6\n",
      "22: 7\n",
      "23: 8\n",
      "24: 9\n",
      "25: :\n",
      "26: ;\n",
      "27: <\n",
      "28: =\n",
      "29: >\n",
      "30: ?\n",
      "31: @\n",
      "32: A\n",
      "33: B\n",
      "34: C\n",
      "35: D\n",
      "36: E\n",
      "37: F\n",
      "38: G\n",
      "39: H\n",
      "40: I\n",
      "41: J\n",
      "42: K\n",
      "43: L\n",
      "44: M\n",
      "45: N\n",
      "46: O\n",
      "47: P\n",
      "48: Q\n",
      "49: R\n",
      "50: S\n",
      "51: T\n",
      "52: U\n",
      "53: V\n",
      "54: W\n",
      "55: X\n",
      "56: Y\n",
      "57: Z\n",
      "58: [\n",
      "59: \\\n",
      "60: ]\n",
      "61: ^\n",
      "62: _\n",
      "63: `\n",
      "64: a\n",
      "65: b\n",
      "66: c\n",
      "67: d\n",
      "68: e\n",
      "69: f\n",
      "70: g\n",
      "71: h\n",
      "72: i\n",
      "73: j\n",
      "74: k\n",
      "75: l\n",
      "76: m\n",
      "77: n\n",
      "78: o\n",
      "79: p\n",
      "80: q\n",
      "81: r\n",
      "82: s\n",
      "83: t\n",
      "84: u\n",
      "85: v\n",
      "86: w\n",
      "87: x\n",
      "88: y\n",
      "89: z\n",
      "90: {\n",
      "91: |\n",
      "92: }\n",
      "93: ~\n",
      "94: �\n",
      "95: �\n",
      "96: �\n",
      "97: �\n",
      "98: �\n",
      "99: �\n",
      "100: �\n",
      "101: �\n",
      "102: �\n",
      "103: �\n",
      "104: �\n",
      "105: �\n",
      "106: �\n",
      "107: �\n",
      "108: �\n",
      "109: �\n",
      "110: �\n",
      "111: �\n",
      "112: �\n",
      "113: �\n",
      "114: �\n",
      "115: �\n",
      "116: �\n",
      "117: �\n",
      "118: �\n",
      "119: �\n",
      "120: �\n",
      "121: �\n",
      "122: �\n",
      "123: �\n",
      "124: �\n",
      "125: �\n",
      "126: �\n",
      "127: �\n",
      "128: �\n",
      "129: �\n",
      "130: �\n",
      "131: �\n",
      "132: �\n",
      "133: �\n",
      "134: �\n",
      "135: �\n",
      "136: �\n",
      "137: �\n",
      "138: �\n",
      "139: �\n",
      "140: �\n",
      "141: �\n",
      "142: �\n",
      "143: �\n",
      "144: �\n",
      "145: �\n",
      "146: �\n",
      "147: �\n",
      "148: �\n",
      "149: �\n",
      "150: �\n",
      "151: �\n",
      "152: �\n",
      "153: �\n",
      "154: �\n",
      "155: �\n",
      "156: �\n",
      "157: �\n",
      "158: �\n",
      "159: �\n",
      "160: �\n",
      "161: �\n",
      "162: �\n",
      "163: �\n",
      "164: �\n",
      "165: �\n",
      "166: �\n",
      "167: �\n",
      "168: �\n",
      "169: �\n",
      "170: �\n",
      "171: �\n",
      "172: �\n",
      "173: �\n",
      "174: �\n",
      "175: �\n",
      "176: �\n",
      "177: �\n",
      "178: �\n",
      "179: �\n",
      "180: �\n",
      "181: �\n",
      "182: �\n",
      "183: �\n",
      "184: �\n",
      "185: �\n",
      "186: �\n",
      "187: �\n",
      "188: \u0000\n",
      "189: \u0001\n",
      "190: \u0002\n",
      "191: \u0003\n",
      "192: \u0004\n",
      "193: \u0005\n",
      "194: \u0006\n",
      "195: \u0007\n",
      "196:\n",
      "197: \t\n",
      "198: \n",
      "\n",
      "199: \u000b\n",
      "200: \f\n",
      "201: \n",
      "202: \u000e\n",
      "203: \u000f\n",
      "204: \u0010\n",
      "205: \u0011\n",
      "206: \u0012\n",
      "207: \u0013\n",
      "208: \u0014\n",
      "209: \u0015\n",
      "210: \u0016\n",
      "211: \u0017\n",
      "212: \u0018\n",
      "213: \u0019\n",
      "214: \u001a\n",
      "215: \u001b\n",
      "216: \u001c\n",
      "217: \u001d\n",
      "218: \u001e\n",
      "219: \u001f\n",
      "220:  \n",
      "221: \n",
      "222: �\n",
      "223: �\n",
      "224: �\n",
      "225: �\n",
      "226: �\n",
      "227: �\n",
      "228: �\n",
      "229: �\n",
      "230: �\n",
      "231: �\n",
      "232: �\n",
      "233: �\n",
      "234: �\n",
      "235: �\n",
      "236: �\n",
      "237: �\n",
      "238: �\n",
      "239: �\n",
      "240: �\n",
      "241: �\n",
      "242: �\n",
      "243: �\n",
      "244: �\n",
      "245: �\n",
      "246: �\n",
      "247: �\n",
      "248: �\n",
      "249: �\n",
      "250: �\n",
      "251: �\n",
      "252: �\n",
      "253: �\n",
      "254: �\n",
      "255: �\n",
      "256:  t\n",
      "257:  a\n",
      "258: he\n",
      "259: in\n",
      "260: re\n",
      "261: on\n",
      "262:  the\n",
      "263: er\n",
      "264:  s\n",
      "265: at\n",
      "266:  w\n",
      "267:  o\n",
      "268: en\n",
      "269:  c\n",
      "270: it\n",
      "271: is\n",
      "272: an\n",
      "273: or\n",
      "274: es\n",
      "275:  b\n",
      "276: ed\n",
      "277:  f\n",
      "278: ing\n",
      "279:  p\n",
      "280: ou\n",
      "281:  an\n",
      "282: al\n",
      "283: ar\n",
      "284:  to\n",
      "285:  m\n",
      "286:  of\n",
      "287:  in\n",
      "288:  d\n",
      "289:  h\n",
      "290:  and\n",
      "291: ic\n",
      "292: as\n",
      "293: le\n",
      "294:  th\n",
      "295: ion\n",
      "296: om\n",
      "297: ll\n",
      "298: ent\n",
      "299:  n\n"
     ]
    }
   ],
   "source": [
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for i in range(300):\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    print(f\"{i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Building the Vocabulary\n",
    "\n",
    "The goal of the BPE tokenization algorithm is to build a vocabulary of commonly occurring subwords, such as:\n",
    "\n",
    "- `298: ent` (which can be found in *entangle*, *entertain*, *enter*, *entrance*, *entity*, etc.).\n",
    "\n",
    "## 1.3 BPE Algorithm Outline\n",
    "\n",
    "### 1. Identify Frequent Pairs\n",
    "\n",
    "- In each iteration, scan the text to find the most commonly occurring pair of bytes (or characters).\n",
    "\n",
    "### 2. Replace and Record\n",
    "\n",
    "- Replace that pair with a new placeholder ID (one not already in use).\n",
    "  - Example: If we start with `0...255`, the first placeholder would be `256`.\n",
    "- Record this mapping in a lookup table.\n",
    "- The size of the lookup table is a **hyperparameter**, also called the **\"vocabulary size\"**.\n",
    "  - (For GPT-2, this is **50,257**.)\n",
    "\n",
    "### 3. Repeat Until No Gains\n",
    "\n",
    "- Keep repeating steps **1 and 2**, continually merging the most frequent pairs.\n",
    "- Stop when no further compression is possible (e.g., no pair occurs more than once).\n",
    "\n",
    "### 4. Decompression (Decoding)\n",
    "\n",
    "- To restore the original text, **reverse the process** by substituting each ID with its corresponding pair using the lookup table.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.4 BPE Algorithm Example\n",
    "\n",
    "### 1.4.1 Concrete Example of the Encoding Part (Steps 1 & 2)\n",
    "\n",
    "#### **Suppose we have the text (training dataset):**  \n",
    "\n",
    "```\n",
    "the cat in the hat\n",
    "```\n",
    "\n",
    "From this text, we want to build the vocabulary for a BPE tokenizer.\n",
    "\n",
    "---\n",
    "\n",
    "### **Iteration 1**\n",
    "\n",
    "#### **Identify Frequent Pairs**  \n",
    "- In this text, `\"th\"` appears **twice** (at the beginning and before the second `\"e\"`).\n",
    "\n",
    "#### **Replace and Record**  \n",
    "- Replace `\"th\"` with a new token ID that is not already in use, e.g., `256`.\n",
    "- The new text becomes:\n",
    "\n",
    "  ```\n",
    "  <256>e cat in <256>e hat\n",
    "  ```\n",
    "\n",
    "- The updated vocabulary:\n",
    "\n",
    "  ```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Iteration 2**\n",
    "\n",
    "#### **Identify Frequent Pairs**  \n",
    "- In the text `<256>e cat in <256>e hat`, the pair `<256>e` appears **twice**.\n",
    "\n",
    "#### **Replace and Record**  \n",
    "- Replace `<256>e` with a new token ID that is not already in use, e.g., `257`.\n",
    "- The new text becomes:\n",
    "\n",
    "  ```\n",
    "  <257> cat in <257> hat\n",
    "  ```\n",
    "\n",
    "- The updated vocabulary:\n",
    "\n",
    "  ```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "  257: \"<256>e\"\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Iteration 3**\n",
    "\n",
    "#### **Identify Frequent Pairs**  \n",
    "- In the text `<257> cat in <257> hat`, the pair `<257> ` appears **twice** (once at the beginning and once before `\"hat\"`).\n",
    "\n",
    "#### **Replace and Record**  \n",
    "- Replace `<257> ` with a new token ID that is not already in use, e.g., `258`.\n",
    "- The new text becomes:\n",
    "\n",
    "  ```\n",
    "  <258>cat in <258>hat\n",
    "  ```\n",
    "\n",
    "- The updated vocabulary:\n",
    "\n",
    "  ```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "  257: \"<256>e\"\n",
    "  258: \"<257> \"\n",
    "  ```\n",
    "\n",
    "And so forth...\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4.2 Concrete Example of the Decoding Part (Step 3)\n",
    "\n",
    "To restore the original text, we **reverse the process** by substituting each token ID with its corresponding pair in the reverse order they were introduced.\n",
    "\n",
    "#### **Starting with the final compressed text:**  \n",
    "```\n",
    "<258>cat in <258>hat\n",
    "```\n",
    "\n",
    "#### **Substitutions (in reverse order):**\n",
    "1. Substitute `<258> → <257> `  \n",
    "   ```\n",
    "   <257> cat in <257> hat\n",
    "   ```\n",
    "2. Substitute `<257> → <256>e`  \n",
    "   ```\n",
    "   <256>e cat in <256>e hat\n",
    "   ```\n",
    "3. Substitute `<256> → \"th\"`  \n",
    "   ```\n",
    "   the cat in the hat\n",
    "   ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
