{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "825ef288",
   "metadata": {},
   "source": [
    "# Demystifying DeepSeekMath’s Data Pipeline: A FastText-Based Reproduction and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee4082f",
   "metadata": {},
   "source": [
    "## 1. Why Data Collection Matters (and Often Gets Overlooked)\n",
    "\n",
    "When you read most LLM papers, the emphasis falls on model architecture, parameter counts, or fine‐tuning tricks. Yet DeepSeekMath shows that where and how you gather your pretraining data in a domain like mathematics (would work in most other fields) can make or break performance. In fact, DeepSeekMath-Base 7B, after training on its carefully curated math corpus, outperformed much larger models on competition‐level benchmarks, simply by focusing on high‐quality data rather than scaling parameters alone.\n",
    "\n",
    "Intuitively, mathematical content on the open web is rare relative to “generic” web pages. Moreover, math pages often embed formulas, LaTeX snippets, or domain‐specific jargon that simple keyword‐based filtering (“does the page contain the word ‘integral’?,” etc.) will miss or misclassify. Consequently, constructing a clean, rich, math‐focused web corpus requires:\n",
    "\n",
    "1. **An initial seed of bona fide math pages**, so that a classifier can learn what math looks like.\n",
    "2. **Iterative expansion**: find new math‐y pages that aren’t in your seed, then retrain the classifier to improve recall.\n",
    "3. **Domain‐level reasoning**: identify entire websites or subpaths (e.g., `mathoverflow.net/questions`) that are math‐intensive.\n",
    "4. **Deduplication and contamination‐filtering**: avoid repeatedly scraping the same content or accidentally including test‐set questions (e.g., GSM8K problems).\n",
    "5. **Token‐budget management**: since “math” pages can be verbose (lots of symbols, proofs, code snippets), decide concretely how many tokens you want from each page to fill your overall 100 B+ token goal.\n",
    "\n",
    "This multi‐stage, multi‐iteration approach is what gave DeepSeekMath its edge: by the fourth round of data collection, they amassed **120 B math‐related tokens** from **35.5 million web pages**—all rigorously filtered for true mathematical content and decontaminated of benchmark questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bd5894",
   "metadata": {},
   "source": [
    "## 2. DeepSeekMath’s Data‐Collection Pipeline (2.1 of the Paper)\n",
    "\n",
    "Let’s unpack the paper’s main “Data Collection and Decontamination” steps, focusing on the highlights you’ll want to mirror or adapt in your own code:\n",
    "\n",
    "### Initial Seed (OpenWebMath)\n",
    "\n",
    "DeepSeekMath begins with OpenWebMath (a curated collection of high‐quality math web text, ≈ 13.6 B tokens).\n",
    "\n",
    "- They randomly sample 500 K math pages from OpenWebMath as positive examples.  \n",
    "- For negative examples, they sample 500 K random pages from the (unfiltered) Common Crawl dump.  \n",
    "\n",
    "These combined 1 million examples (500 K math / 500 K non‐math) train a fastText classifier with hyperparameters:  \n",
    "`dim=256, lr=0.1, wordNgrams=3, minCount=3, epoch=3`.\n",
    "\n",
    "---\n",
    "\n",
    "### Iterative Recall of Math Pages (4 Rounds)\n",
    "\n",
    "After training the classifier on seed data, they run it over a URL‐deduplicated Common Crawl corpus of 40 B HTML pages.  \n",
    "Any page that scores above a certain threshold is tentatively labeled “math.”  \n",
    "\n",
    "- They sort by classifier score and preserve only the top chunk (e.g., the highest‐scoring pages that together amount to 40 B tokens on the first pass).  \n",
    "- **Domain‐based expansion**: They group Common Crawl pages by domain (e.g., all pages at `mathoverflow.net`) and check what fraction were already “collected” in the previous iteration.  \n",
    "    - If an entire domain has > 10% of its pages labeled as math, that domain is flagged as math‐related.  \n",
    "    - They then hand‐annotate the specific URL paths that truly contain math (e.g., `/questions`).  \n",
    "    - Any Common Crawl URLs under those paths that weren’t previously selected are added to the seed.  \n",
    "\n",
    "This yields a richer set of positives for retraining the classifier.  \n",
    "\n",
    "They repeat this four times. By the end of Round 4, nearly 98% of new math pages have already been found, so they stop.  \n",
    "The result is **35.5 million math pages totaling 120 B tokens**.\n",
    "\n",
    "---\n",
    "\n",
    "### Deduplication (URL‐level & MD5 Sketching)\n",
    "\n",
    "- Before classification, they run URL‐based deduplication to collapse trivial redirects or mirrored pages.  \n",
    "- After fetching the HTML, they do a near‐duplicate check via MD5 on the first 3,000 characters of each page’s text.  \n",
    "    - If two pages share the same 3,000‐char MD5 hash, one is dropped.\n",
    "\n",
    "---\n",
    "\n",
    "### HTML→Plain Text\n",
    "\n",
    "Once a page is fetched (WARC segment):  \n",
    "- They strip all tags (regex `<[^>]+>` → spaces) and collapse whitespace.  \n",
    "- This yields a coarse “plain text” version to feed to the classifier.\n",
    "\n",
    "---\n",
    "\n",
    "### Benchmark Contamination Filtering\n",
    "\n",
    "- They remove any page that contains a 10‐gram substring appearing in GSM8K, MATH, CMATH, etc.  \n",
    "    - For shorter n‐grams (≥ 3), they do exact matching.  \n",
    "\n",
    "This ensures their pretraining data doesn’t leak test problems.\n",
    "\n",
    "---\n",
    "\n",
    "### Token‐Budget Selection\n",
    "\n",
    "- Each candidate page yields an estimated token count (via their tokenizer).  \n",
    "- They rank pages by classifier confidence and keep adding pages until they hit 120 B tokens (stopping when they exceed budget).  \n",
    "\n",
    "This “greedy by confidence” method ensures the highest‐quality pages fill the budget before including lower‐confidence ones.\n",
    "\n",
    "---\n",
    "\n",
    "## How This Creates a “High-Quality Math Corpus”\n",
    "\n",
    "- **Seed → Classifier → Recall** iteratively refines what “math” means to the model.  \n",
    "- **Domain flags** (e.g., `mathoverflow.net`) catch entire sites that a pure page‐classifier might miss.  \n",
    "- **Deduplication** ensures you don’t waste tokens on near‐identical content.  \n",
    "- **N-gram filtering** removes test‐set contamination.  \n",
    "- **Confidence-driven token budget** prioritizes truly math-heavy pages first.  \n",
    "\n",
    "All together, this pipeline produces a math corpus that is:  \n",
    "\n",
    "- **Large**: 120 B tokens.  \n",
    "- **Multilingual**: Although English dominates, they also keep Chinese math pages (e.g., Gaokao problems).  \n",
    "- **Clean**: No GSM8K/MATH question leaks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071941d4",
   "metadata": {},
   "source": [
    "## 3. Your Python Code: A Single-Pass, FastText-Based Math Scraper\n",
    "\n",
    "Below is the Python code that implements a simplified version of the DeepSeekMath data collection pipeline using FastText for classification. This code will help you train a FastText classifier on a math dataset, scrape web pages, and filter them based on the classifier's predictions.\n",
    "\n",
    "### 3.1. Training a FastText Classifier on a Math Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea46ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Install required packages\n",
    "%pip install datasets -q\n",
    "%pip install cdx-toolkit -q\n",
    "%pip install warcio fasttext tqdm tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bcd8794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import json\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import re\n",
    "import hashlib\n",
    "import tqdm\n",
    "from datasets import load_dataset\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "import cdx_toolkit\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e696c97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2- Load the dataset\n",
    "# https://huggingface.co/datasets/kenhktsui/math-classifiers-data\n",
    "ds = load_dataset(\"kenhktsui/math-classifiers-data\") # you should probably spend some time to understand the dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86790b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3- Write out training & validation files in fastText format\n",
    "with open(\"math.train\", \"w\") as f:\n",
    "    for example in ds['train']:\n",
    "\n",
    "        label = example['label']\n",
    "        label_str = f\"__label__{label}\" \n",
    "        f.write(f\"{label_str} {example['text']}\\n\")\n",
    "\n",
    "with open(\"math.valid\", \"w\") as f:\n",
    "    for example in ds['test']:\n",
    "\n",
    "        label = example['label']\n",
    "        label_str = f\"__label__{label}\"\n",
    "        f.write(f\"{label_str} {example['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a799b99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 1165M words\n",
      "Number of words:  5814943\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread: 6477088 lr:  0.000000 avg.loss:  0.117713 ETA:   0h 0m 0s avg.loss:  0.117713 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# 4- Train with hyperparameters matching DeepSeekMath (dim=256, lr=0.1, wordNgrams=3, minCount=3, epoch=3)\n",
    "model = fasttext.train_supervised(\n",
    "    input=\"math.train\",\n",
    "    lr=0.1,\n",
    "    dim=256,\n",
    "    wordNgrams=3,\n",
    "    epoch=3,\n",
    "    minCount=3,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 5- Save the model to disk\n",
    "model.save_model(\"model/math-classifier.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f732c5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: (1600004, 0.9890106524733688, 0.9890106524733688)\n",
      "Valid metrics: (400004, 0.9722727772722273, 0.9722727772722273)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# 6- Evaluate quickly on train/valid\n",
    "math = fasttext.load_model(\"model/math-classifier.bin\")\n",
    "print(\"Train metrics:\", math.test(\"math.train\"))\n",
    "print(\"Valid metrics:\", math.test(\"math.valid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f82853d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label for 'What is the integral of x^2 ?': (('__label__0',), array([1.00001001]))\n",
      "Label for 'in the politics of the United States, what is the role of the president?': (('__label__1',), array([0.95782489]))\n"
     ]
    }
   ],
   "source": [
    "print(\"Label for 'What is the integral of x^2 ?':\", math.predict(\"What is the integral of x^2 ?\"))\n",
    "print(\"Label for 'in the politics of the United States, what is the role of the president?':\", math.predict(\"in the politics of the United States, what is the role of the president?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba2111d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size estimate for query: 21000\n"
     ]
    }
   ],
   "source": [
    "# --- 3.2: Build a small CDX index (1 000 HTML pages) in pure Python ---\n",
    "\n",
    "import cdx_toolkit\n",
    "import pandas as pd\n",
    "\n",
    "cdx = cdx_toolkit.CDXFetcher(source=\"cc\")\n",
    "query = \"commoncrawl.org/*\"\n",
    "print(\"Size estimate for query:\", cdx.get_size_estimate(query))\n",
    "\n",
    "rows = []\n",
    "for obj in cdx.iter(query, limit=1000, filter=[\"status:200\", \"mime:text/html\"]):\n",
    "    rows.append((obj[\"url\"], obj[\"filename\"], obj[\"offset\"], obj[\"length\"]))\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"url\", \"warc\", \"offset\", \"length\"])\n",
    "df.to_csv(\"cc-index.csv\", sep=\",\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f18ae42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [07:35<00:00,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched & scored 544 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3.3: Fetch each WARC segment, dedupe, strip HTML, classify with fastText (pure Python) ---\n",
    "\n",
    "# 1) Load the tiny 1,000-row index\n",
    "columns = [\"url\", \"warc\", \"offset\", \"length\"]\n",
    "rows = pd.read_csv(\"cc-index.csv\", sep=\",\", names=columns, dtype=str)\n",
    "\n",
    "\n",
    "\n",
    "seen_url = set()\n",
    "seen_sim = set()\n",
    "scored = []\n",
    "\n",
    "for _, row in tqdm.tqdm(rows.iterrows(), total=len(rows)):\n",
    "    url = row[\"url\"]\n",
    "    warc_filename = row[\"warc\"]\n",
    "    offset = int(row[\"offset\"])\n",
    "    length = int(row[\"length\"])\n",
    "\n",
    "    warc_url = f\"https://data.commoncrawl.org/{warc_filename}\"\n",
    "    byte_range = f\"bytes={offset}-{offset + length - 1}\"\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(warc_url, headers={\"Range\": byte_range}, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    for rec in ArchiveIterator(io.BytesIO(resp.content)):\n",
    "        if rec.rec_type != \"response\":\n",
    "            continue\n",
    "        try:\n",
    "            html = rec.content_stream().read().decode(\"utf-8\", \"ignore\")\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # 1) URL-level dedupe\n",
    "        if url in seen_url:\n",
    "            continue\n",
    "        seen_url.add(url)\n",
    "\n",
    "        # 2) Near-dup via MD5 on first 3,000 chars\n",
    "        md5_prefix = hashlib.md5(html[:3000].encode(\"utf-8\", \"ignore\")).hexdigest()\n",
    "        if md5_prefix in seen_sim:\n",
    "            continue\n",
    "        seen_sim.add(md5_prefix)\n",
    "\n",
    "        # 3) Strip tags → plain-ish text\n",
    "        text = re.sub(r\"<[^>]+>\", \" \", html)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        # 4) Classify with fastText\n",
    "        label, prob = math.predict(text)\n",
    "        scored.append({\n",
    "            \"url\": url,\n",
    "            \"text\": text,\n",
    "            \"label\": label[0],\n",
    "            \"score\": float(prob[0])\n",
    "        })\n",
    "\n",
    "print(f\"Fetched & scored {len(scored)} pages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "215fe038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 544 docs → 809,936 tokens (~0.81 M)\n"
     ]
    }
   ],
   "source": [
    "# --- 3.4: Count tokens and select pages until 1 M token budget is reached ---\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "TOKENS_BUDGET = 1_000_000\n",
    "\n",
    "for doc in scored:\n",
    "    doc[\"ntok\"] = len(enc.encode(doc[\"text\"]))\n",
    "\n",
    "scored.sort(key=lambda d: d[\"score\"], reverse=True)\n",
    "\n",
    "kept = []\n",
    "total_tokens = 0\n",
    "for doc in scored:\n",
    "    if total_tokens + doc[\"ntok\"] > TOKENS_BUDGET:\n",
    "        break\n",
    "    kept.append(doc)\n",
    "    total_tokens += doc[\"ntok\"]\n",
    "\n",
    "print(f\"Kept {len(kept)} docs → {total_tokens:,} tokens (~{total_tokens/1e6:.2f} M)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26338af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote pass1-math.jsonl 👍\n"
     ]
    }
   ],
   "source": [
    "# --- 3.5: Write selected pages to JSONL (pure Python) ---\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"pass1-math.jsonl\", \"w\", encoding=\"utf-8\") as out:\n",
    "    for doc in kept:\n",
    "        out.write(json.dumps({\n",
    "            \"url\": doc[\"url\"],\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"label\": doc[\"label\"],\n",
    "            \"score\": doc[\"score\"]\n",
    "        }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Wrote pass1-math.jsonl 👍\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a67eecc",
   "metadata": {},
   "source": [
    "## 4. Recommendations for a “Full-Scale” Version\n",
    "\n",
    "If you want to scale from 1 M tokens all the way up to 120 B tokens (and truly match DeepSeekMath), here are the concrete steps I would add:\n",
    "\n",
    "### Pre-Fetch URL Deduplication\n",
    "- Maintain a `seen_urls` set and skip any URL before requesting its WARC.\n",
    "\n",
    "### HTML Parsing Instead of Regex\n",
    "- Use `BeautifulSoup` to drop `<script>`, `<style>`, `<nav>`, `<footer>` tags, then extract `<article>` or the largest `<div>` by text length.\n",
    "\n",
    "### Benchmark N-Gram Filtering\n",
    "- Build an Aho–Corasick automaton of all 10-grams from GSM8K/MATH/CMATH. Drop pages that match any of these.\n",
    "\n",
    "### Domain Feature & Manual Annotation\n",
    "- Track `(domain_name → (#pages_seen, #pages_labelled_math))`.\n",
    "- After each pass, flag domains where >10% of pages were classified as math.\n",
    "- Hand-annotate URL patterns for those domains and add them into the next round’s seed.\n",
    "\n",
    "### Iterative Classifier Retraining\n",
    "- After each pass, split your “kept” pages into high-confidence positives, low-confidence candidates, and negatives.\n",
    "- Label a random subset of low-confidence pages, retrain `fastText` on the expanded seed, and re-run.\n",
    "- Repeat until gains diminish.\n",
    "\n",
    "### Token Budgeted Collection (Two-Tiered)\n",
    "- First estimate `approx_tokens = len(text.split())` to skip large pages when near budget.\n",
    "- Then compute exact `len(tiktoken.encode(text))` only for borderline candidates.\n",
    "\n",
    "### Sharding & Output\n",
    "- When you write `pass1-math.jsonl`, split into 128 or 256 shards by `hash(url) % N`.\n",
    "- Also build a CSV `index_of_shards.csv` mapping each URL to its shard & byte offset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c64f56",
   "metadata": {},
   "source": [
    "## 5. Final Thoughts & Conclusion\n",
    "\n",
    "Building a 120 B-token, high-quality math corpus is nontrivial. You have to:\n",
    "\n",
    "- **Start with a good seed**: Use OpenWebMath or a similarly curated dataset.\n",
    "- **Train a strong classifier**: FastText is a good choice if speed is a priority, ensuring diversity in positives and negatives.\n",
    "- **Iterate**: Recall, expand domains, retrain, and recall again—scrubbing contaminated pages at each pass.\n",
    "- **Deduplicate ruthlessly**: Perform deduplication at both the URL level and via near-duplicate hashing.\n",
    "- **Budget tokens**: Aim for 120 B tokens, not 10 trillion. Prioritize the most trustworthy pages.\n",
    "- **Shard your final output**: Ensure your pretraining pipeline can scale effectively.\n",
    "\n",
    "In my demo script, I implemented a mini DeepSeekMath pipeline:\n",
    "\n",
    "- FastText → tiny CDX slice (1,000 pages) → MD5 deduplication → strip tags → classify → greedy 1 M token selection → JSONL.\n",
    "\n",
    "This provides a taste of the mechanics.\n",
    "\n",
    "However, to match the paper’s performance, you’d need to implement the full iterative, domain expansion, and contamination filtering steps.\n",
    "\n",
    "DeepSeekMath demonstrated that all a 7 B model needed to solve competition-level MATH (51.7% top-1 accuracy) was good data—not 540 B parameters like Minerva. As the open-source community, we can replicate this success by carefully building domain-specific corpora for other fields.\n",
    "\n",
    "I hope this guide helps you appreciate “data collection” as a first-class research direction—perhaps the unsung hero behind every high-performing LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd527bc",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Zhihong Shao et al. “DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.” *arXiv:2402.03300* (27 Apr 2024).  \n",
    "    [https://arxiv.org/abs/2402.03300](https://arxiv.org/abs/2402.03300)\n",
    "\n",
    "2. Hugging Face Dataset: Math Classifiers Data  \n",
    "    [https://huggingface.co/datasets/kenhktsui/math-classifiers-data](https://huggingface.co/datasets/kenhktsui/math-classifiers-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6189db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ipynb to markdown\n",
    "import nbformat\n",
    "def convert_ipynb_to_md(ipynb_path, md_path):\n",
    "    with open(ipynb_path, 'r', encoding='utf-8') as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "    \n",
    "    with open(md_path, 'w', encoding='utf-8') as f:\n",
    "        for cell in nb.cells:\n",
    "            if cell.cell_type == 'markdown':\n",
    "                f.write(cell.source + '\\n\\n')\n",
    "            elif cell.cell_type == 'code':\n",
    "                f.write('```python\\n' + cell.source + '\\n```\\n\\n')\n",
    "convert_ipynb_to_md(\"math-classifier.ipynb\", \"math-classifier.md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
